# Análisis de la Legislación en Trámite sobre IA en Chile

## 1. Antecedentes Políticos y Técnicos del Proyecto de Ley

El marco legal en trámite surge de la **fusión de dos iniciativas** legislativas: (1) un **Mensaje presidencial** ingresado el 7 de mayo de 2024 (boletín Nº 16.821-19), y (2) una **moción parlamentaria** presentada el 24 de abril de 2023 (boletín Nº 15.869-19) por un grupo transversal de diputadas y diputados. La moción original buscaba regular **los sistemas de inteligencia artificial, la robótica y tecnologías conexas**, reflejando la preocupación del Congreso (especialmente de la Comisión de Futuro, Ciencia, Tecnología e Innovación) por los avances en IA desde 2023. Durante 2023, dicha Comisión realizó un trabajo preparatorio extenso **estudiando la IA** a partir de esa moción inicial, sentando bases técnicas y recogiendo insumos de expertos para la futura ley.

El Mensaje del Ejecutivo de 2024 dotó al proyecto de **urgencia suma** y enmarcó la necesidad de la ley tanto en compromisos del programa de gobierno como en tendencias internacionales. En el texto de exposición de motivos, el Gobierno destaca la rápida evolución de la IA y su impacto transversal, ofreciendo *“extraordinario potencial”* para el bienestar y competitividad, pero también nuevos **riesgos y desafíos éticos**. Se citan referentes globales como la **Recomendación UNESCO 2021 sobre ética de la IA**, y esfuerzos regulatorios como la **Ley de IA de la Unión Europea** – adoptando un enfoque basado en niveles de riesgo (inaceptable, alto, limitado, mínimo) – que sirvió de inspiración para la clasificación propuesta en Chile. Asimismo, se menciona la aproximación de Estados Unidos centrada en autorregulación y compromisos voluntarios de las grandes empresas tecnológicas, además de avances en China en regulación de algoritmos y contenido sintético.

En síntesis, desde el punto de vista **político**, el proyecto responde al interés nacional por **impulsar la IA de forma responsable**, alineado con estándares internacionales y la estrategia país de transformación digital. Desde lo **técnico**, incorpora principios éticos globales y lecciones comparadas para **garantizar certezas regulatorias** que permitan el desarrollo e implementación de la IA **respetando los derechos fundamentales**. Cabe destacar que la iniciativa fue remitida a la **Excma. Corte Suprema** para informe, dada la posible incidencia en materias judiciales: vía Oficio de 8 de mayo de 2024 se le consultó sobre ciertas disposiciones, recibiéndose respuesta el 21 de junio 2024. El proyecto ingresó con **apoyo político transversal**, reflejado en su aprobación unánime en general por la Comisión especializada.

## 2. Estructura y Contenido del Proyecto de Ley de IA

El proyecto de ley refundido (boletines 15.869-19 y 16.821-19) establece un **marco jurídico integral** para regular los distintos usos de sistemas de inteligencia artificial. Consta, en su versión original, de **31 artículos permanentes y 3 transitorios**, organizados en títulos temáticos. A grandes rasgos, los títulos abarcan: **Disposiciones Generales** (objeto, ámbito de aplicación, definiciones, clasificación de riesgos), **Sistemas de Riesgo Inaceptable**, **Sistemas de IA de Alto Riesgo**, **Sistemas de IA de Riesgo Limitado**, **Incidentes Graves**, **Gobernanza de IA**, **Medidas de Apoyo a la Innovación**, **Confidencialidad, Infracciones y Sanciones**, y **Disposiciones Finales y Transitorias**. A continuación se detalla su contenido más relevante:

* **Objeto y Ámbito de Aplicación (Art. 1-2):** La ley tiene por objeto *“promover la creación, desarrollo, innovación e implementación de sistemas de IA al servicio del ser humano”* de manera coherente con principios democráticos, Estado de Derecho y derechos fundamentales. Se aplica a **todos los actores de la cadena de valor de sistemas de IA**: desarrolladores/proveedores que introduzcan o desplieguen IA en Chile, implementadores que las utilicen, importadores, distribuidores y representantes locales de proveedores extranjeros, **cuando las salidas del sistema se usen en territorio nacional**. Este criterio de aplicación territorial por uso busca asegurar que siempre haya un responsable local del uso de IA en Chile. La ley **excluye explícitamente** ciertos ámbitos: (a) los sistemas de IA **desarrollados y usados con fines de defensa nacional** (cuyos detalles serán listados en una resolución reservada del Ministerio de Defensa); (b) las actividades de **investigación, pruebas y desarrollo previas a la comercialización** de sistemas de IA, **mientras respeten los derechos fundamentales** y sin que involucren pruebas en condiciones reales (cualquier daño ocurrido en esa etapa experimental igualmente genera responsabilidad civil); y (c) los **componentes de IA de licencia libre o código abierto**, salvo que sean comercializados como parte de un sistema de IA de alto riesgo por un proveedor, en cuyo caso sí quedan sujetos a la ley. También se exceptúa el *“uso privado”* de un sistema de IA por una persona (definición de implementador excluye uso personal no comercial). Estas exclusiones han sido objeto de atención especial, buscando equilibrar la promoción de la innovación (ej. protección de la investigación y el software abierto) con la prevención de vacíos regulatorios.

* **Definiciones Clave (Art. 3):** El proyecto entrega un glosario de definiciones para precisar su alcance. Define **“Sistema de IA”** como *sistema basado en máquinas que infiere, a partir de entradas, ciertas salidas (predicciones, contenidos, recomendaciones o decisiones) capaces de influir en entornos físicos o virtuales*, abarcando distintos niveles de autonomía y adaptabilidad. Se definen también conceptos de **riesgo** y **riesgo significativo** (combinación de probabilidad y gravedad del daño que puede afectar a personas). Importante es la noción de **operadores de IA**, que agrupa a proveedores, implementadores, importadores, distribuidores y representantes autorizados, así como la definición de **persona afectada** (quien sufre un perjuicio por la IA). Estas definiciones transversales delinean quiénes son sujetos obligados por la ley. *(Nota:* En discusiones posteriores se incorporó además la definición de **“sistema de IA de uso general”**, para referirse a aquellas IA de propósito general que pueden tener múltiples aplicaciones, dada la necesidad de tratarlas de forma especial en el marco normativo.)

* **Principios Rectores (Art. 4):** La ley consagra una serie de **principios éticos y operativos** que deberán observar los *operadores* de sistemas de IA. Estos principios orientan tanto el desarrollo como el uso de la IA, y sirven de base interpretativa de las obligaciones. Entre los **principios generales de la IA** enumerados en el artículo 4º se incluyen:

* **Intervención y supervisión humana:** la IA debe estar *“al servicio de las personas”*, respetar la dignidad y autonomía humana, y contar con posibilidad de control y supervisión adecuada por seres humanos.

* **Solidez y seguridad técnica:** los sistemas deben minimizar daños previsibles, ser técnicamente robustos frente a fallas imprevistas o usos maliciosos, garantizando la seguridad de las personas.

* **Privacidad y gobernanza de datos:** deben desarrollarse y usarse conforme a las normas vigentes de protección de datos personales, incorporando interoperabilidad y seguridad en el manejo de datos.

* **Transparencia y explicabilidad:** los sistemas de IA deben ser *transparentes en su funcionamiento* y sus procesos de decisión **explicables** a los usuarios; es decir, las personas deben poder comprender cómo se generan las conclusiones o recomendaciones de la IA. *(Este principio de “Explicabilidad” fue reforzado en indicaciones posteriores, añadiéndose explícitamente como principio autónomo.)*

* **Diversidad, no discriminación y equidad:** la IA debe ser diseñada y utilizada promoviendo la igualdad de oportunidades, la inclusión (por ejemplo, equidad de género y diversidad cultural) y evitando sesgos o impactos discriminatorios ilegítimos.

* **Bienestar social y medioambiental:** considerar los impactos positivos en la sociedad y el medio ambiente, usando IA de forma sostenible y respetuosa, evaluando efectos a largo plazo en estas dimensiones.

* **Rendición de cuentas y responsabilidad:** quienes diseñan, desarrollan, operan o despliegan IA deben asegurar el **correcto funcionamiento durante todo el ciclo de vida** de los sistemas, asumiendo responsabilidad por sus funciones y resultados.

* **Protección de los derechos de los consumidores:** los sistemas de IA deben respetar las normas de protección al consumidor, asegurando un trato **justo, información veraz y transparente, libertad de elección y seguridad** en el consumo de tecnologías basadas en IA.

Estos principios no son meras declaraciones: guían la implementación de la ley. De hecho, se dispone que tanto el **Ministerio de Ciencia, Tecnología, Conocimiento e Innovación (CTCI)** como la **Agencia de Protección de Datos Personales (APDP)** – en sus respectivas competencias – incorporarán dichos principios al orientar, regular y fiscalizar el desarrollo y uso de IA. *(Más adelante, una indicación agregaría también a la* *Agencia Nacional de Ciberseguridad (ANCI)* *en esta labor coordinada de emisión de orientaciones para operadores de IA.)*

* **Clasificación de los Sistemas de IA por Nivel de Riesgo (Art. 5):** El proyecto adopta un **enfoque regulatorio basado en el riesgo**, similar al modelo europeo. El artículo 5º establece cuatro **categorías de riesgo** para los sistemas de IA, definiéndolas brevemente y determinando consecuencias regulatorias para cada una:

* **Riesgo Inaceptable:** Son sistemas de IA *“incompatibles con el respeto y garantía de los derechos fundamentales”*, cuyo uso se considera contrario al ordenamiento jurídico nacional. **Están prohibidos** absolutamente (ver detalles en art. 6). Esta categoría captura usos de IA que por su naturaleza podrían vulnerar gravemente la dignidad, la seguridad o derechos básicos de las personas.

* **Alto Riesgo:** Sistemas de IA cuyo uso **presenta un riesgo significativo** de causar perjuicios a la salud, seguridad, derechos fundamentales o al medio ambiente, *entre otros*. A diferencia de los anteriores, **no se prohíben** pero **su uso queda condicionado** al cumplimiento estricto de una serie de **requisitos obligatorios** de confiabilidad y seguridad (ver art. 7-10). La identificación específica de qué sistemas se consideran "de alto riesgo" se delega a **reglamento del Ministerio CTCI**, elaborado previa propuesta del Consejo Asesor de IA, conforme a los criterios establecidos en la ley. Es decir, la ley fija la definición general y criterios, pero la lista concreta se actualizará vía reglamentaria.

* **Riesgo Limitado:** Sistemas de IA cuyo uso presenta *“riesgos de manipulación o engaño no significativos”*. Podrían generar resultados incorrectos o sesgados que, sin ser altamente dañosos, pueden inducir error en las personas. No se prohibe su uso; la ley simplemente les impone **reglas de transparencia básicas** – principalmente, la obligación de informar a las personas que están interactuando con una IA. Se busca así mitigar los riesgos leves de desinformación o confusión.

* **Sin Riesgo Evidente o Mínimo:** Corresponde a sistemas de IA cuyo uso *no presenta riesgos significativos*. La ley no les impone requisitos especiales más allá de observar los principios generales. Se entiende que la mayoría de sistemas cotidianos de bajo impacto caen en esta categoría, por lo que quedan **liberados de obligaciones adicionales**.

*Nota:* Estas categorías son mutuamente excluyentes y se aplican a los **“usos” específicos de la IA más que a la tecnología en abstracto**. Un mismo sistema podría considerarse de alto riesgo en cierto contexto de uso, pero de riesgo limitado en otro. Este enfoque flexible se subraya en el proyecto, que habla de *“uso de sistemas de IA de alto riesgo”* en vez de etiquetar permanentemente a una herramienta. De hecho, una indicación ajustó la redacción de los títulos para recalcar que es la **utilización** la que se califica con nivel de riesgo (por ejemplo, cambiando el epígrafe a “Uso de riesgo alto de sistemas de IA”).

* **Sistemas de IA de Riesgo Inaceptable – Prohibiciones (Art. 6):** El artículo 6º enumera **taxativamente las categorías de usos de IA prohibidos** por implicar un riesgo inaceptable para la sociedad. Estas prohibiciones buscan prevenir aplicaciones de IA que atenten gravemente contra derechos humanos, seguridad de las personas o valores éticos fundamentales. En la versión actual del proyecto (tras indicaciones), las categorías de **“usos de IA de riesgo inaceptable”** incluyen las siguientes **prácticas proscritas**:

* **a) Manipulación subliminal:** Sistemas de IA que empleen técnicas imperceptibles para inducir comportamientos en las personas que resulten en daño a su salud física o mental. *Excepción:* Quedan fuera de esta prohibición sistemas de IA con fines terapéuticos, siempre que medie **consentimiento informado, específico y expreso** de las personas expuestas (o de su representante legal, según corresponda) y cuenten con la autorización sanitaria pertinente. Esto permite, por ejemplo, el uso de ciertas tecnologías neurológicas o de estimulación en tratamiento médico, bajo estricta supervisión y consentimiento.

* **b) Explotación de vulnerabilidades o características de personas para inducir comportamientos dañinos:** Sistemas de IA que aprovechan características particulares de individuos o grupos (rasgos de personalidad, situación socioeconómica, edad, orientación sexual, identidad de género, capacidad física/mental, etc.) con el objeto de alterar sustancialmente su comportamiento o menoscabar su voluntad, provocando perjuicios o vulnerando derechos fundamentales. En esta categoría se deja explícito que incluye aquellos usos de IA que generen daño a la honra, integridad o libre desarrollo sexual de las personas – en particular afectando derechos de **niños, niñas y adolescentes**, de acuerdo con la Ley Nº21.430.

* **c) Categorización masiva de personas basada en datos sensibles:** Sistemas de IA que clasifiquen o **perfilen personas según datos personales sensibles** (ej. biometría, salud, convicciones, origen étnico) o inferencias sobre esos atributos, de modo tal que la categorización resultante provoque discriminación ilegal o arbitraria. *Excepción:* Al igual que en (a), se exceptúan sistemas de IA destinados a fines terapéuticos autorizados con consentimiento informado específico y aprobación sanitaria, si correspondiere.

* **d) Calificación social generalizada (“social scoring”):** Sistemas de IA que evalúan o clasifican personas o grupos en función de su comportamiento social, situación económica, características personales o de personalidad – sean datos reales o inferidos – y producen una **puntuación social** que derive en un trato discriminatorio ilegal o arbitrario hacia esas personas. En esencia, se prohíbe la creación de sistemas de **crédito social o rankings ciudadanos** de tipo general que afecten derechos (similar a evitar sistemas estilo “social credit”).

* **e) Identificación biométrica remota en espacios públicos en tiempo real:** Sistemas de IA utilizados para analizar imágenes de video de lugares públicos con **identificación biométrica** (p.ej. reconocimiento facial) **en tiempo real**. *Excepción importante:* Esta prohibición **no aplica cuando tales sistemas sean utilizados estrictamente por autoridades u órganos de seguridad pública y perseguimiento penal**, con el objetivo de prevenir o investigar delitos graves, o ejecutar sanciones penales, conforme a la ley. Es decir, **se permite su uso policial** y por fiscales bajo marco legal, pero se veta su uso indiscriminado en el ámbito privado o con otros fines en espacios abiertos. Esta excepción ha sido materia de debate dado el potencial impacto en la privacidad ciudadana.

* **f) Extracción masiva indiscriminada de imágenes faciales (“facial scraping”):** Sistemas de IA que crean o amplían bases de datos de reconocimiento facial mediante la extracción **no selectiva** de imágenes desde internet o de cámaras de CCTV. Con esta prohibición se busca frenar prácticas de recopilación a gran escala de rostros sin consentimiento (por ejemplo, para entrenar algoritmos comerciales), las cuales han generado preocupación a nivel global.

* **g) Evaluación de emociones en ámbitos sensibles:** Sistemas de IA diseñados para **inferir los estados emocionales** de una persona en contextos como la aplicación de la ley penal (investigación, procesos), la gestión de fronteras, el ámbito laboral o centros educativos. En tales entornos, la ley prohíbe emplear IA para determinar, por ejemplo, si alguien está nervioso, mintiendo, etc., dada la falta de base científica robusta y la posible invasión de la privacidad mental.

Estas categorías de riesgo inaceptable reflejan las prácticas identificadas con *alto potencial de manipular o lesionar a las personas*. Todas ellas **quedan prohibidas en Chile**. Cabe destacar que algunas incluyeron salvedades muy puntuales (en (a) y (c) para usos médicos, en (e) para seguridad pública) a fin de no entorpecer ciertos fines legítimos, pero siempre bajo condiciones estrictas (consentimiento informado, autorización sanitaria, base legal, etc.). En general, el enfoque es **prohibir por defecto** estos usos, permitiendo excepciones solo cuando exista un interés público mayor y salvaguardas específicas.

* **Sistemas de IA de Alto Riesgo – Obligaciones y Control (Arts. 7 a 10):** Los sistemas cuyo **uso se califique como de “alto riesgo”** no están prohibidos, pero quedan sujetos a un conjunto de **requisitos obligatorios** destinados a asegurar que operen de forma segura, confiable y ética. El artículo 7 define conceptualmente cuándo un uso de IA se considera de alto riesgo (riesgo significativo para derechos fundamentales, salud, seguridad, medioambiente o derechos de consumidores). También señala que, tratándose de sistemas de alto riesgo, **“deberán prevenir la creación de estereotipos o la degradación de personas o grupos”** mediante su operación – subrayando la preocupación por evitar sesgos discriminatorios. La tarea de precisar **qué sistemas específicos son de alto riesgo** recae en un futuro **Reglamento** dictado por el Ministerio de Ciencia, con propuesta del Consejo Asesor de IA, lo que permitirá actualizar la lista conforme evolucione la tecnología.

**Reglas obligatorias:** El artículo 8 (y siguientes) detalla las **obligaciones que deben cumplir los operadores de sistemas de IA de alto riesgo**, cubriendo todo el ciclo de vida del sistema. Tras las modificaciones introducidas en Comisión, estas obligaciones quedaron estructuradas así:

* **Gestión de Riesgos:** Implementar un **sistema de gestión de riesgos continuo e iterativo** durante todo el ciclo de vida del IA. Esto implica evaluaciones periódicas para minimizar fallas o malfuncionamientos conforme a la finalidad prevista del sistema. En caso de que un sistema de alto riesgo no cumpla con las reglas aplicables, el operador deberá **desactivarlo, retirarlo o suspender su uso de inmediato**. (Esta última obligación antes estaba en un artículo separado, pero se incorporó como cláusula general de cierre en las reglas de alto riesgo.)

* **Gobernanza de Datos:** Garantizar una **adecuada gobernanza de los datos** utilizados en el entrenamiento y operación del sistema, acorde a su propósito y contexto de uso. Las técnicas de IA basadas en aprendizaje requieren asegurarse de la calidad y pertinencia de los datos. Tras las indicaciones, se explicitó que deben incorporarse **estándares de seguridad y protección de datos**, incluyendo **mecanismos de prevención y gestión de incidentes de seguridad de la información**, según el ámbito de aplicación.

* **Documentación Técnica:** Contar con documentación técnica **inteligible** y suficiente que demuestre el cumplimiento de las reglas de la ley. La información técnica del sistema de IA de alto riesgo debe estar disponible de modo transparente para acreditar su seguridad y conformidad. (Una indicación menor ajustó la redacción para clarificar que es la documentación *requerida para* el sistema, en lugar de “que acompañe al” sistema.)

* **Registros (Logs):** Los sistemas de alto riesgo deberán tener funcionalidades para **registrar información y eventos** durante su funcionamiento. Además, dichos registros deben almacenarse con **medidas de seguridad adecuadas** para evitar alteración, pérdida o accesos no autorizados, y su acceso estará limitado a personal autorizado y a la autoridad fiscalizadora competente. Esta trazabilidad permite investigar fallas o incidentes y asignar responsabilidades.

* **Transparencia y Explicabilidad:** Los sistemas de IA de alto riesgo deben ser diseñados con **un nivel suficiente de transparencia y explicabilidad** de modo que los operadores y destinatarios puedan *“entender razonablemente el funcionamiento”* del sistema conforme a su finalidad. También, los usuarios deben poder **identificar que están interactuando con un sistema de IA**, salvo que ello resulte evidente por el contexto. Adicionalmente, desde el momento en que el sistema se introduce en el mercado, se deben utilizar todos los medios técnicos disponibles conforme al estado del arte para posibilitar que los operadores **interpreten las salidas del sistema**. En resumen, se exige que estos sistemas no sean cajas negras impenetrables, sino que ofrezcan cierto grado de explicabilidad y anuncio de su naturaleza artificial. *(Cabe señalar que en las discusiones se reforzó esta obligación integrando explícitamente el concepto de “explicabilidad” y la notificación a usuarios.)*

* **Supervisión Humana:** Deben contar con **mecanismos técnicos y operativos de supervisión humana** apropiados. Es decir, personas capacitadas deben poder monitorear el funcionamiento del sistema y tener capacidad de intervención cuando sea necesario. La supervisión debe garantizar que el sistema se use conforme a su propósito previsto y permitir identificar/mitigar riesgos de un uso indebido razonablemente previsible. Esto apunta a prevenir resultados nocivos mediante intervención humana oportuna.

* **Precisión, Solidez y Ciberseguridad:** Los sistemas de alto riesgo han de ser desarrollados bajo el principio de **seguridad desde el diseño y por defecto**, con un nivel adecuado de **precisión, confiabilidad, robustez y ciberseguridad**, funcionando de manera estable a lo largo de su ciclo de vida. Las indicaciones complementaron que el sistema debe operar de forma *fiable, predecible y resiliente*, garantizando seguridad y resistencia a incidentes. Asimismo, se exige que el cumplimiento de estos requisitos se garantice mediante la implementación de **medidas de seguridad alineadas con la Ley Nº21.663 (Marco de Ciberseguridad)**, en particular sus artículos 3, 7 y 9. Esto integra las normas de ciberseguridad nacionales, reconociendo que muchos sistemas de IA serán parte de infraestructuras críticas o procesos sensibles donde la ciberseguridad es clave.

Finalmente, el marco permite cierta **flexibilidad proporcional**: el reglamento podrá establecer **estándares diferenciados** para cumplir estas reglas según el tipo y tamaño del operador, con especial consideración a las pequeñas y medianas empresas. De esta manera se busca no ahogar la innovación de startups o empresas menores con cargas excesivas, aplicando el principio de proporcionalidad. Por ejemplo, requisitos técnicos complejos podrían adaptarse para mipymes manteniendo los objetivos de seguridad.

**Mecanismos de control:** Además de las obligaciones anteriores, la ley prevé un **deber de monitoreo continuo**: los implementadores de IA de alto riesgo deberán establecer sistemas de **seguimiento posterior a la comercialización** para recabar información de desempeño e incidentes una vez el sistema esté en uso, y así detectar nuevas amenazas o necesidades de ajuste. Este es un *“post-market monitoring”* similar al de dispositivos médicos, para mejorar la seguridad con la retroalimentación de la operación real. Si pese a todo un sistema de alto riesgo llegare a fallar en cumplir las reglas mencionadas, el operador debe tomar medidas inmediatas para apagarlo, retirarlo o recuperarlo del mercado. Esta obligación de *“kill switch”* asegura que ningún sistema de IA peligroso siga operando una vez detectado el incumplimiento.

* **Sistemas de IA de Riesgo Limitado – Transparencia (Art. 11):** El artículo 11 caracteriza a los sistemas de **riesgo limitado** como aquellos cuyo uso conlleva un riesgo *no significativo* de manipulación, engaño o error en la interacción con personas. Para estos sistemas, la ley establece básicamente un **deber de transparencia hacia los usuarios**: se debe procurar que los sistemas de IA de riesgo limitado **se ofrezcan en condiciones que informen clara y precisamente a las personas** que están frente a una tecnología de IA. En otras palabras, el usuario debe ser *notificado de que interactúa con una máquina* (por ejemplo, un chatbot debe revelarse como tal). Esta obligación tiene una **excepción**: no regirá cuando se trate de sistemas de IA **autorizados por ley para fines de detección, prevención, investigación o enjuiciamiento penal**. Es decir, si una ley permite emplear una IA en labores policiales o investigativas encubiertas, no sería necesario alertar al investigado que hay IA involucrada. Fuera de ese caso puntual, la transparencia es mandatoria incluso para IA de riesgo bajo, con el fin de resguardar el derecho a saber de los ciudadanos.

* **Incidentes Graves de IA (Art. 13):** Dado que incluso sistemas supervisados pueden fallar, el proyecto define el concepto de **“incidente grave”** de IA en sus definiciones (Art. 3) e instituye un mecanismo de reporte. El artículo 13 dispone que **cualquier persona que identifique un incidente grave** relacionado con un sistema de IA podrá reportarlo a la **Agencia a cargo de la Protección de Datos Personales (APDP)**. La APDP, al recibir el informe, **deberá informar al operador** responsable para que éste adopte cuanto antes las medidas de **información y contención** correspondientes. De este modo se crea una especie de *sistema de alerta temprana* ante fallos o eventos adversos significativos (p.ej., un sesgo discriminatorio descubierto, una brecha de seguridad, un accidente causado por IA), involucrando al regulador de datos como punto central de recepción. La finalidad es *“mitigar daños previstos e imprevistos, especialmente en sistemas de alto riesgo, y restaurar su funcionamiento normal”* con celeridad. Además, mediante indicaciones se reforzó la **coordinación con la Agencia de Ciberseguridad (ANCI)** en esta materia: si el incidente reportado involucra infraestructura crítica o operadores vitales según la ley 21.663 de ciberseguridad, la APDP deberá notificar y coordinar con la ANCI para su evaluación y respuesta. Esto asegura que incidentes graves con componente de seguridad informática reciban el tratamiento especializado debido.

* **Gobernanza Institucional de la IA (Arts. 14 a 18):** Un eje central del proyecto es la creación de arreglos institucionales para la **gobernanza de la IA en Chile**. Se contemplan dos pilares: **un Consejo Asesor Técnico en IA** de carácter consultivo, y la **atribución de fiscalización y sanción a la Agencia de Protección de Datos Personales**.

* **Consejo Asesor Técnico de IA:** El artículo 14 crea el *“Consejo Asesor Técnico de Inteligencia Artificial”* como un organismo **permanente y consultivo** en materias de IA. Tendrá por función principal asesorar al o la Ministra de Ciencia, Tecnología, Conocimiento e Innovación (CTCI) en la implementación de esta ley. El artículo 15 enumera sus **principales funciones**, entre ellas: **(i)** proponer al Ministro(a) CTCI el listado de sistemas de IA de alto riesgo y de riesgo limitado (según criterios de la ley); **(ii)** asesorar al Ministerio CTCI en el alcance y modo de cumplimiento de las reglas obligatorias para operadores de IA de alto y limitado riesgo; **(iii)** presentar propuestas para establecer **lineamientos sobre espacios controlados de prueba (sandboxes)** y estándares mínimos para dichos entornos de experimentación segura. En el segundo paquete de indicaciones (marzo 2025) se decidió **ampliar las funciones** del Consejo: se le añadió la tarea de **asesorar a distintos Ministerios** respecto al cumplimiento de las obligaciones de esta ley en sus sectores, y la de **colaborar en la promoción de la alfabetización y divulgación ciudadana en materia de IA**. Es decir, el Consejo también impulsará formación y conocimiento público sobre IA.

* En cuanto a su **integración**, los artículos 16 a 18 regulan aspectos de la conformación del Consejo, las inhabilidades para ser consejero, causales de cesación y normas de funcionamiento. Sin entrar en detalle, cabe mencionar que el Consejo estará compuesto por expertos en la materia; por ejemplo, se prevé la participación de representantes de sectores público, académico (incluyendo académicos/as expertos) y eventualmente privados, buscando un enfoque multidisciplinario. (Durante la tramitación se ajustó lenguaje para asegurar inclusión de consejeros/as de ambos sexos en ciertas cuotas, cambiando “académico experto” por “académico/a experto/a”, etc., en las indicaciones).

* Importante: el Consejo Asesor es **consultivo**, no tiene potestades resolutivas directas. No obstante, su influencia será significativa al proponer la lista de IA de alto riesgo y lineamientos técnicos. Se optó por esta figura para contar con asesoramiento especializado y flexible sin crear un nuevo ente regulador con atribuciones vinculantes (lo cual fue debatido, pero finalmente las funciones fiscalizadoras se dejaron en la Agencia de Datos).

* **Autoridad de Fiscalización y Cumplimiento – APDP:** El proyecto asigna la **supervisión del cumplimiento de la ley** a la **Agencia encargada de la Protección de Datos Personales (APDP)**. Esto significa que la APDP (una agencia creada por la reciente Ley de Protección de Datos Personales, aún en fase de instalación) será la autoridad encargada de velar porque los operadores de IA acaten las prohibiciones y obligaciones establecidas. Entre sus facultades estarán: **(i)** fiscalizar las disposiciones de la ley y su reglamento; **(ii)** determinar infracciones e incumplimientos de quienes contravengan prohibiciones u obligaciones; **(iii)** ejercer la potestad sancionadora imponiendo multas u otras sanciones administrativas; **(iv)** resolver los reclamos que las personas afectadas interpongan contra operadores que vulneren la ley. En la práctica, la APDP actuará como el **“ente regulador de IA”** en lo relativo a enforcement, análogo a lo que en la UE sería la autoridad de supervisión que vigila la aplicación del reglamento de IA.

* Dada la complejidad técnica de la IA, el proyecto fomenta que la APDP coordine su labor con otras entidades especializadas. Ya mencionamos la coordinación con la **ANCI** en casos de ciberseguridad (incidentes graves y operadores críticos). Asimismo, en la etapa de indicaciones de agosto 2025, se **ratificó la competencia del Consejo para la Transparencia** en materias de transparencia y acceso a la información pública relacionadas con la IA. Esto implica que si un sistema de IA es usado por un organismo público, las obligaciones de transparencia activa o de respuesta a solicitudes de información siguen siendo fiscalizadas por el Consejo para la Transparencia conforme a la ley de acceso a la información, evitando solapamientos con la APDP. En resumen, la APDP será el brazo ejecutor sancionatorio, pero deberá trabajar en conjunto con ANCI en seguridad e informar al Consejo de Transparencia en lo relativo a publicidad de información.

* **Medidas de Apoyo a la Innovación en IA (Arts. 20 a 22):** Reconociendo la importancia de **promover el desarrollo de IA** además de regular riesgos, el proyecto incorpora un título específico con **medidas de fomento e innovación**. En particular, se faculta la creación de **“espacios controlados de prueba”** o *sandboxes regulatorios* para IA. Estos sandboxes permiten que órganos públicos autoricen pilotos o pruebas experimentales de sistemas de IA en entornos restringidos y supervisados, con el fin de probar innovaciones sin incurrir de inmediato en todas las obligaciones legales, pero bajo controles que protejan a los participantes. Las indicaciones aclararon que **el uso de un sandbox es voluntario** – *“no es un requisito habilitante”* para desplegar una IA en el mercado – y que participar en un sandbox **no exime al operador de cumplir las obligaciones de la ley**. Es decir, los sandboxes se conciben como facilitadores, no como vacíos legales. Asimismo, en las indicaciones finales se precisó que los órganos del Estado que habiliten sandboxes podrán **facilitar acceso temporal, restringido y auditado a datos** en formato seguro, para efectos de fomentar la innovación en IA. Esto apunta a uno de los obstáculos habituales para entrenar IA (la falta de datos de calidad), permitiendo que, bajo condiciones estrictas, startups o investigadores accedan a ciertos datasets públicos relevantes.

Adicionalmente, se incluyen medidas orientadas a **pymes y startups**: por ejemplo, programas de apoyo técnico y financiero para adopción de IA en empresas de menor tamaño, y consideraciones especiales en la aplicación de obligaciones (como se mencionó antes, diferenciación de estándares). El objetivo es que la regulación no sea sólo restrictiva, sino que también impulse el ecosistema nacional de IA.

* **Confidencialidad, Infracciones y Régimen Sancionatorio (Arts. 23 a 27):** En el título de fiscalización y sanciones, el artículo 23 establece deberes de **confidencialidad**: toda información o datos obtenidos por autoridades o entidades en ejercicio de las funciones de esta ley (por ejemplo, durante auditorías a un sistema de IA) deberá resguardarse y usarse sólo para esos fines. Esto protege secretos comerciales o datos sensibles de empresas evaluadas, dando garantías para la colaboración con el regulador.

Los artículos 24 al 27 disponen el **catálogo de infracciones administrativas** y las **sanciones** aplicables por la APDP en caso de incumplimiento de la ley. Se tipifican distintas conductas infractoras – desde introducir o usar sistemas prohibidos, hasta no implementar las medidas obligatorias para IA de alto riesgo, o no colaborar con la autoridad – clasificándolas posiblemente en leves, graves y muy graves (según trascendió en discusión, aunque el texto detallado de infracciones está en los documentos). Las sanciones incluyen **multas administrativas** cuyas cuantías máximas se definieron en unidades tributarias. De acuerdo con antecedentes presentados a la Comisión, las infracciones muy graves podrían sancionarse hasta con **20.000 UTM** y las graves hasta 10.000 UTM. Por ejemplo, 20.000 UTM equivalen a alrededor de 1.200 millones de pesos, mostrando que se prevén multas elevadas para conductas de alto impacto. La ley establece además criterios de graduación de la pena (agravantes, atenuantes) y un **procedimiento administrativo sancionador** que deberá seguir la APDP, con garantías procesales para el afectado. Se prevé un proceso con instancias de descargos, posibilidad de aportar pruebas, etc., y luego la resolución fundada de la Agencia imponiendo la sanción. Contra esa resolución, el infractor podrá recurrir ante la justicia (reclamación judicial) para revisión independiente.

Un punto a destacar es la preocupación por la **proporcionalidad de las sanciones** en el caso de pequeñas empresas. Durante la tramitación, parlamentarios advirtieron que una multa aparentemente razonable (ej. 5.000 UTM) podría ser absolutamente ruinosa para una startup, solicitando mecanismos para asegurar que se apliquen criterios de proporcionalidad y posibilidad de ajuste según el tamaño del infractor. El proyecto al respecto contempla – además de la diferenciación de estándares de cumplimiento – que la autoridad sancionadora considere todas las circunstancias del caso, incluyendo la capacidad económica del infractor, para evitar multas desproporcionadas.

* **Responsabilidad Civil por Daños de IA (Arts. 28 y 29):** Más allá de las sanciones administrativas, la ley introduce reglas sobre **responsabilidad civil extracontractual** por daños causados por sistemas de IA. Los artículos 28 y 29 regulan una acción civil por culpa contra el operador de un sistema de IA cuando su uso genere daño indemnizable. Se establece la posibilidad de demandar por perjuicios derivados de la IA, precisando aspectos procesales (por ejemplo, inversión o carga dinámica de la prueba, según lo discutido). Este punto requirió revisión por la Corte Suprema, ya que incide en la competencia y organización de los tribunales civiles. De hecho, el artículo 28 original tenía carácter de ley orgánica constitucional al afectar atribuciones del Poder Judicial, y su aprobación requiere quorum calificado. En la votación general de sala de la Cámara, este artículo 28 fue **rechazado en general** por no alcanzar el quorum constitucional, quedando pendiente su reanálisis. Posiblemente se refiera a reglas especiales (como la intervención de ciertos tribunales en estas causas o limitaciones de responsabilidad) que generaron dudas. Las indicaciones han ido ajustando estos artículos para armonizarlos con el derecho común civil y las observaciones del Poder Judicial.

* **Disposiciones Finales y Transitorias:** El artículo 30 y siguientes incluyen modificaciones a otras leyes y normas de implementación. En particular, se introduce una **enmienda a la Ley Nº17.336 de Propiedad Intelectual** para favorecer la IA: se incorpora un nuevo artículo (71 T) que establece una **excepción al derecho de autor** que permite la **extracción, análisis o minería de datos** (textos, sonidos, imágenes u obras) a gran escala para fines de desarrollo de IA, *“en la medida que dicho uso no constituya una explotación encubierta de obras protegidas”*. Esta excepción de *text and data mining* busca facilitar el entrenamiento de algoritmos de IA sobre conjuntos de datos u obras, modernizando la normativa de propiedad intelectual chilena en línea con tendencias internacionales. Fue un aspecto muy valorado por la comunidad de IA, aunque observado con cautela por titulares de derechos de autor.

Las normas transitorias establecen la **vacatio legis** (entrada en vigencia) y plazos para dictar los reglamentos y actos administrativos necesarios para implementar la ley. Por ejemplo, se fija que el reglamento principal (sobre sistemas de alto y limitado riesgo, y contenido de las reglas técnicas) deberá dictarse dentro de cierto plazo por el Ministerio CTCI. También se contemplan transitorios para la instalación del Consejo Asesor de IA y, crucialmente, para coordinar con la entrada en vigor de la nueva ley de datos personales (que crea la APDP). Es de esperar que haya una disposición que indique que, mientras la Agencia no esté operativa, sus funciones las asumirá transitoriamente otra entidad (posiblemente el Servicio Nacional de Protección de Datos una vez creado). En la **cuarta disposición transitoria**, según el Informe Financiero, se establece un plazo y financiamiento para la implementación – esta disposición debió ser conocida por la Comisión de Hacienda por implicar gasto fiscal (se estima que la implementación de la ley generará costos, por ejemplo, por la creación del Consejo y fortalecimiento de la APDP).

En resumen, el proyecto de ley presenta una **arquitectura robusta** que combina definiciones claras, principios éticos, enfoque basado en riesgo con categorías reguladas proporcionalmente, prohibiciones de usos inadmisibles, obligaciones técnicas para sistemas riesgosos, mecanismos de reporte de incidentes, creación de una gobernanza institucional (Consejo Asesor y Agencia fiscalizadora), sanciones disuasivas y medidas de fomento a la innovación. Todo ello con la mira de **aprovechar las oportunidades de la IA promoviendo su desarrollo**, pero **resguardando a la sociedad de sus posibles efectos nocivos**.

## 3. Análisis de las Indicaciones Presentadas al Proyecto

A lo largo de la tramitación, el Ejecutivo y parlamentarios han introducido **indicaciones (enmiendas)** para perfeccionar el texto. En particular, existen tres hitos de indicaciones presentadas por el Gobierno en fechas clave: **18 de octubre de 2024**, **4 de marzo de 2025** y **19 de agosto de 2025**, además de indicaciones de parlamentarios durante el debate en sala. A continuación se sintetiza el contenido y motivación de estas indicaciones, conforme a los documentos proporcionados:

* **Indicaciones del 18 de octubre de 2024:** Correspondieron a un primer paquete de enmiendas del Ejecutivo durante la discusión en Comisión, de carácter acotado. Dos cambios destacados fueron: (1) introducir en el artículo 15 (sobre funciones del Consejo Asesor de IA) un inciso para aclarar que los **“sistemas de IA de uso general”** deberán **cumplir siempre con las obligaciones del artículo 8** de la ley. Con esto se buscó no dejar fuera del marco a las IA de propósito general (como grandes modelos de lenguaje) que, por ser multipropósito, pudieran eludir exigencias – así, se asegura que *las IA generales también implementen las reglas de gestión de riesgo, transparencia, etc.* cuando sean utilizadas; (2) agregar en el artículo 21 (espacios de prueba) que *“la utilización de un sandbox no es requisito habilitante para la introducción en el mercado de un sistema de IA ni exime de las obligaciones establecidas en esta ley”*. Esta indicación reafirma que el régimen de **sandboxes regulatorios es voluntario y no suspende la aplicación de las normas**: una empresa puede lanzar una IA sin pasar por un sandbox, y si pasa por uno igualmente debe cumplir la ley. Ambos cambios respondieron a dudas técnicas: el primero, ante la emergencia de **modelos fundacionales** (general AI) y la necesidad de encuadrarlos; el segundo, para despejar interpretaciones erróneas sobre la figura del sandbox (evitando que se entienda como vía obligatoria o como “territorio libre” fuera de la ley).

* **Indicaciones del 4 de marzo de 2025:** Este fue un **conjunto sustancial de indicaciones** formuladas por el Ejecutivo tras un amplio trabajo de la Comisión y consulta con diversos actores. Las enmiendas del 04-03-2025 introdujeron modificaciones de fondo al articulado, muchas de ellas alineadas con las recomendaciones de la Corte Suprema y con perfeccionar la armonización con estándares internacionales. Entre los cambios más relevantes están:

* **Refuerzo de Principios:** Se añadió un **nuevo principio rector, “Explicabilidad”**, en la lista del artículo 4º, literal k), consagrando explícitamente que los sistemas de IA se desarrollarán y usarán de manera que sus resultados sean comprensibles para las personas afectadas, promoviendo transparencia y trazabilidad. Aunque la idea ya estaba implícita, al elevarlo a principio se subraya su importancia. Asimismo, se reemplazó el inciso final del art. 4 para detallar la incorporación de los principios por parte de los órganos competentes: ahora señala que **MCTCI, la APDP y la ANCI** incorporarán estos principios en sus orientaciones al operador, en la regulación y fiscalización dentro de sus competencias, reconociendo formalmente el rol de ciberseguridad de ANCI junto al de datos personales de la APDP.

* **Redefinición de Usos de Riesgo Inaceptable (Art. 6):** Se **reemplazó completamente el texto del artículo 6** para detallar mejor cada categoría prohibida, alineándolas con la redacción del borrador de la **AI Act europea**. Por ejemplo, se describió con mayor precisión *“Manipulación subliminal”* (incorporando explícitamente la referencia a daño a salud física o mental); en *“Explotación de vulnerabilidades”* se enumeraron características personales aprovechables (rasgos de personalidad, situación económica, edad, orientación sexual, identidad de género, etc.) y se subrayó la protección de NNA y derechos sexuales, remitiendo a la ley de garantías de la niñez; para *“Categorización por datos sensibles”* se especificó que incluye técnicas biométricas u otras y se aclara la excepción terapéutica con consentimiento. Se añadió una categoría explícita de *“Extracción no selectiva de imágenes faciales”* (facial scraping) como literal f), que en la versión original quizás no figuraba tan clara. También se ajustó la excepción del reconocimiento facial en tiempo real para uso policial, limitándola a utilización **estricta** por autoridades de seguridad pública y persecución penal, y *“conforme a la ley”* (reforzando que debe existir base legal para cada uso, p. ej. orden judicial o ley de vigilancia). Finalmente, se precisó que la prohibición de *“evaluación emocional”* (literal g) aplica en ámbitos penal, laboral y educacional. En suma, las indicaciones de marzo 2025 **afinaron el catálogo de prohibiciones** para que no queden lagunas ni ambigüedades, dando mayor certeza sobre qué prácticas son ilícitas.

* **Clarificaciones en la Clasificación de Riesgos (Art. 5 y Títulos):** Se modificó el título III para que se llame “Uso de riesgo alto” en lugar de solo “Sistemas de IA de alto riesgo”, enfatizando el concepto de *uso*. Se reescribió el **artículo 7** definiendo “uso de IA de alto riesgo” con una redacción más concisa: se considerará alto riesgo cuando el uso del sistema **presente un riesgo significativo de afectar derechos fundamentales**, incluyendo si el sistema es componente de seguridad de un producto o un producto en sí mismo. Se eliminó quizá la enumeración original de ejemplos o detalles redundantes. Además, se agregó en art. 7 que el uso de IA de alto riesgo deberá procurar el respeto de derechos fundamentales y prevenir estereotipos o degradación de personas, incorporando consideraciones éticas directamente en el articulado operativo. Estos cambios refuerzan la obligación general de cuidado en usos de alto riesgo más allá de las meras reglas técnicas.

* **Obligaciones de Alto Riesgo (Art. 8 y 9):** Hubo **numerosas modificaciones al artículo 8**, tanto de forma como de fondo, para mejorar la coherencia y completitud de las reglas:

  * Se ajustó el **encabezado** del art. 8 para que introduzca claramente la lista de reglas (“*Los sistemas de IA cuyos usos sean calificados de alto riesgo deberán cumplir con las siguientes reglas relativas a:* ...”).

  * **Riesgo vs. incidente:** En el literal a) (gestión de riesgos) se cambió la palabra “contingencia” por “incidente”[[191][192]](file://file-9x7mzANiJpL8qwfDvK2WKc#:~:text=b,a), para unificar la terminología con el resto de la ley (que habla de “incidentes” graves).

  * **Gobernanza de datos:** Se reemplazó el literal b) con una definición más completa: requiere que sistemas de IA entrenados con datos cuenten con una **gobernanza de datos adecuada a su propósito y contexto**, incorporando estándares de seguridad y protección de datos, y **mecanismos de prevención y gestión de incidentes de seguridad de la información**. Esto responde a la recomendación de robustecer el aspecto de calidad y seguridad de datos, esencial para evitar sesgos y brechas.

  * **Documentación técnica:** En literal c), cambio menor de redacción (“acompañe al” por “requerida para el” sistema) para clarificar que la documentación es la necesaria para operar/demostrar las reglas.

  * **Registros:** Se sustituyó el literal d) por un texto más detallado que describe la función de registros de eventos y especifica las condiciones de almacenamiento seguro, acceso restringido y protección de integridad de esos registros. Es decir, no basta con tener logs: deben guardarse de forma confiable y solo personal autorizado o la autoridad pueden acceder.

  * **Transparencia y explicabilidad:** Se reformuló el literal e) para unir **transparencia y explicabilidad**. Ahora exige tanto transparencia como explicabilidad suficientes para que operadores y usuarios entiendan razonablemente el funcionamiento, y agrega que los usuarios deben poder identificar que interactúan con IA (salvo que sea evidente por contexto). Se suprimió la frase original que obligaba a “interpretar la información de salida” (aunque en realidad se mantuvo en un párrafo aparte) y se enfatizó la identificación frente al usuario. También se eliminó la excepción de transparencia para usos penales en esta sección, ya que se reubicó en principios generales. En suma, se robusteció la exigencia de **explicabilidad y señalización al usuario**.

  * **Supervisión humana:** Literal f) reformulado exige que los sistemas de alto riesgo cuenten con **mecanismos técnicos y operativos para su supervisión humana** por parte de personal capacitado, garantizando que se use conforme a su propósito y que se identifiquen/mitiguen riesgos de uso indebido previsible, evitando impactos en derechos. Esta redacción es más específica que la original, agregando el objetivo de la supervisión (prevenir usos indebidos).

  * **Precisión, resiliencia y ciberseguridad:** Literal g) reescrito para exigir fiabilidad, predictibilidad y resiliencia, garantizando seguridad durante todo el ciclo de vida, e integrando explícitamente el cumplimiento de medidas de seguridad de la Ley de Ciberseguridad 21.663, como ya se comentó. Este cambio conecta la normativa de IA con los estándares nacionales de ciberseguridad, reconociendo la intersección entre ambos campos.

  * **Escala para Pymes:** Se mantuvo la cláusula final (ahora como párrafo separado tras literal g) que permite estándares diferenciados para MYPES en el cumplimiento de las reglas, pero se reubicó y reescribió ligeramente para mayor claridad.

  * **Integración de Art. 9 en Art. 8:** Se decidió **fusionar el contenido del antiguo artículo 9** (acciones ante no conformidad) dentro del propio artículo 8 como párrafo final. Así, tras listar las reglas, se añade: *“Cuando un sistema de IA de alto riesgo no se ajuste a las reglas de la presente ley, el operador adoptará inmediatamente las medidas necesarias para desactivarlo, retirarlo del mercado o suspenderlo”*, indicando además que dichas medidas deben estar previstas dentro del sistema de gestión de riesgos del sistema. Esto evita duplicación y deja claro, dentro del mismo artículo de reglas, la obligación de cese inmediato ante incumplimiento. Se menciona que la APDP y/o la ANCI, en el ámbito de sus competencias, podrán requerir esa desactivación al operador, reforzando la facultad de la autoridad para exigir el retiro. Como consecuencia de esta fusión, todos los artículos posteriores se recorrieron (el art. 10 pasó a ser 9, 11 a ser 10, etc., según las indicaciones).

* **Coordinación APDP–ANCI en incidentes y seguridad:** Se introdujeron nuevas disposiciones para que la APDP coordine con la Agencia de Ciberseguridad en caso de incidentes que afecten infraestructura crítica o “operadores de importancia vital” según la ley de ciberseguridad. Por ejemplo, una indicación añadió como función de la APDP el *“coordinar con la ANCI en los casos en que, en ejercicio de sus facultades, detecte incidentes de seguridad que afecten a operadores vitales”*, estableciendo que la APDP deberá remitir antecedentes a la ANCI y podrá solicitarle informe sobre el incidente. Esto surgió seguramente del aporte de la ANCI o Ministerio del Interior, para asegurar respuesta integral ante ciberincidentes en sistemas de IA críticos.

* **Ajustes de redacción varios:** Además de lo anterior, las indicaciones de marzo 2025 incluyeron múltiples ajustes menores: lenguaje inclusivo (e.g. “académico/a”), correcciones terminológicas (cambiando “contingencia” por “incidente”, etc.), numeraciones (renombrando títulos y artículos conforme a inserciones o fusiones), y mejoras en concordancia con otras leyes (como dejar explícito que las normas de responsabilidad civil que introduce deben seguir las reglas procesales correspondientes, en concordancia con lo señalado por la Corte Suprema).

En conjunto, las indicaciones de 4 de marzo de 2025 refinaron notablemente el proyecto, dotándolo de mayor claridad, tecnicidad y coherencia. Gran parte de estas modificaciones fueron **acogidas por la Comisión**, integrándose al texto del **Informe** para ser considerado por la Cámara.

* **Indicaciones del 19 de agosto de 2025:** Tras la aprobación en general en la Sala de la Cámara (el 4 de agosto de 2025) y previo a la discusión en particular, el Ejecutivo ingresó un **tercer paquete de indicaciones** (código Nº 174-373) para ajustar el proyecto. Estas enmiendas de última hora apuntaron a resolver observaciones finales y preparar el texto para su segundo informe en la Comisión de Futuro. Según el **Informe Financiero Complementario** asociado, las indicaciones de 19-08-2025 realizan principalmente las siguientes precisiones:

* **Enfoque en usos de IA:** Se afina la redacción del *objeto de la ley* para destacar que se regula **el** uso **de sistemas de IA** (no la mera existencia de la tecnología), y se aplica ese criterio en todo el texto. Esto refuerza el principio rector de neutralidad tecnológica y que las obligaciones se activan por cómo se emplea la IA.

* **Definiciones clave aclaradas:** Se aclaran las definiciones de **“sistema de IA”** (posiblemente para alinearla con definiciones internacionales actualizadas) y de **“sistema de IA de uso general”**, incorporando probablemente criterios para identificar a estos últimos. Esto responde a la rápida evolución de los *foundation models* y la necesidad de un tratamiento especial.

* **Transparencia pública:** Se **ratifican las competencias del Consejo para la Transparencia** en lo relativo a transparencia y acceso a la información pública sobre IA. En concreto, se asegura que nada en la nueva ley menoscaba la Ley de Transparencia (20.285) – por ejemplo, que los principios de transparencia en IA se aplicarán sin perjuicio del derecho de acceso a información en poder de órganos públicos. Esto disipó temores de conflictos de competencia entre la APDP y el Consejo de Transparencia.

* **Clasificación de riesgos pulida:** Se introducen **precisiones en la clasificación de los usos de IA**, particularmente en las definiciones de **riesgo inaceptable** y **alto riesgo**, probablemente para hacerlas más objetivas o incorporar criterios adicionales (como alcance del impacto, número de personas afectadas, etc.). También se mencionó una aclaración en la categoría de *manipulación subliminal* dentro de riesgos inaceptables, tal vez para delimitar mejor qué constituye “técnicas imperceptibles” o qué daños quedan comprendidos.

* **Funciones del Consejo Asesor ampliadas:** Se **agregan dos nuevas funciones al Consejo Asesor de IA**. La primera, asesorar a **distintos Ministerios** respecto al cumplimiento de las reglas de IA en sus políticas sectoriales (extendiendo su asesoría más allá de MCTCI). La segunda, **colaborar con el Ministerio CTCI en la promoción de la alfabetización y divulgación ciudadana** en materia de IA. Esta última consolida el rol del Consejo como promotor de una cultura de IA responsable, organizando posiblemente campañas educativas, material de difusión, etc., para que la población comprenda qué es la IA y sus implicancias.

* **Facilidades de acceso a datos en sandboxes:** Como se mencionó, para fomentar la innovación, las indicaciones establecen que los órganos públicos que implementen **espacios controlados de prueba** podrán ofrecer **acceso temporal, restringido y auditable a conjuntos de datos** en formato estructurado, interoperable y seguro. Esto permitirá a desarrolladores de IA probar algoritmos con datos reales gubernamentales (por ejemplo, datasets anonimizados de salud, tráfico, etc.) bajo supervisión, acelerando desarrollos.

* **Alfabetización ciudadana a cargo de autoridades:** Se incorpora también la **función de promover programas de alfabetización y divulgación** en IA a las agencias competentes (posiblemente la APDP o el mismo Ministerio CTCI). Aunque el fragmento provisto no lo detalla completamente, es claro el énfasis en la educación ciudadana para el uso seguro y consciente de la IA.

En resumen, las indicaciones de agosto 2025 son **ajustes finales** que atienden observaciones levantadas en la etapa de discusión general y por informes: subrayan el enfoque en usos (respuesta a potenciales confusiones conceptuales), fortalecen la coordinación institucional (Transparencia, Consejo IA, Ministerios), y añaden compromisos de **educación e interoperabilidad de datos** para apoyar la implementación práctica de la ley. Tras incorporarlas, la Comisión de Futuro quedó en condiciones de emitir su **segundo informe** en la Cámara, para la votación en particular del proyecto.

Además de las indicaciones del Ejecutivo, cabe mencionar una **indicación parlamentaria** relevante presentada en la discusión en Sala el 4 de agosto de 2025 por la diputada Mónica Arce. Dicha indicación proponía, en el artículo 3 (definiciones), **agregar un numeral 16** con una nueva letra f) que incluyera explícitamente entre las definiciones de *incidente grave* la *“vulneración a la ley N°21.430, sobre Garantías y Protección de los Derechos de la Niñez y Adolescencia”*. Es decir, pretendía que cualquier afectación a los derechos de niños/as por un sistema de IA se considere incidente grave. Esta indicación motivó que el proyecto volviera a Comisión para segundo informe, según el Reglamento de la Cámara. Aún no es claro si fue acogida en el informe, pero refleja la preocupación de parlamentarios por **reforzar la protección de la infancia** en el marco de IA (tema que, como vimos, también fue abordado en las indicaciones del Ejecutivo al prohibir usos dañinos contra NNA).

## 4. Puntos Controversiales y Evaluación Crítica

Aunque el proyecto ha concitado **amplio acuerdo en su idea matriz**, en el proceso legislativo han emergido **puntos controversiales** o temas que han requerido mayor deliberación. A continuación, se evalúan críticamente algunos de esos aspectos, sustentados en los documentos:

* **Definición del Ámbito de Aplicación y Exclusiones:** Una discusión relevante ha sido **qué queda dentro y fuera del alcance de la ley**. La exclusión total de los sistemas de IA destinados a la **defensa nacional** ha generado comentarios, pues si bien responde a razones de seguridad estatal, implica que desarrollos de IA militar (por ejemplo, sistemas autónomos de armas o vigilancia militar) no estarán sujetos a las salvaguardas de esta ley. Esto preocupa a quienes abogan por un control ético de la IA militar, aunque se confía en que habrá regulación específica reservada bajo el Ministerio de Defensa. Asimismo, la excepción otorgada a las **actividades de investigación y desarrollo previas a mercado** suscitó debate sobre un posible *vacío legal*: algunos expertos advirtieron que empresas podrían intentar disfrazar implementaciones comerciales como “pruebas” para eludir responsabilidades. No obstante, el proyecto mitigó ese riesgo exigiendo respeto a derechos fundamentales aun en la etapa experimental y aclarando que **pruebas en condiciones reales no están exentas**. Además, cualquier daño causado en fase de prueba igualmente genera responsabilidad civil, lo cual desincentiva abusos de esta excepción. Otra exclusión discutida fue la de **componentes open source**: se quiso proteger el ecosistema de software libre para IA, de modo que meramente publicar un modelo abierto no convierta al desarrollador en responsable bajo la ley. Sin embargo, se previó que si ese componente libre es usado por un tercero en un sistema de alto riesgo comercial, entonces sí se le aplican las obligaciones. Esta salvedad equilibró innovación abierta con responsabilidad en usos críticos. En balance, las exclusiones parecen **justificadas**, pero será clave su correcta implementación: por ejemplo, la coordinación con Defensa para delimitar los sistemas exentos, y la vigilancia de que no se abuse de la etiqueta “en desarrollo” para evadir la ley.

* **Categorías de Riesgo Inaceptable y Excepciones Controversiales:** La lista de **usos prohibidos de IA** ha contado con amplio respaldo, al estar basada en estándares internacionales, pero no sin puntos polémicos. Uno es la **excepción para sistemas de identificación biométrica remota en tiempo real usados por policías**. Organizaciones de derechos digitales han advertido que permitir el reconocimiento facial masivo por fuerzas de orden puede conducir a vigilancia estatal excesiva o discriminatoria, instando a prohibirlo completamente (como propuso inicialmente la UE). El proyecto chileno optó por autorizarlo solo para prevención e investigación de delitos graves, bajo marco legal estricto. Esta excepción fue defendida por motivos de seguridad pública, pero sigue siendo **controversial**: requerirá acompañarse de normativa secundaria muy clara (por ejemplo, límites de uso, retención de datos, supervisión judicial) para evitar violaciones a la privacidad. Otro punto debatido fue la **prohibición de evaluación emocional**. Algunos sectores privados (p.ej. desarrolladores de *affective computing*) señalaron que esa tecnología podría tener usos positivos (en educación personalizada, salud mental, etc.) y que una prohibición absoluta en ámbitos laborales y educacionales podría ser excesiva. Sin embargo, primó la precaución dado lo inmaduro de esas técnicas y su potencial intrusión. Es posible que a futuro, con mayor base científica, se revise esa restricción. En general, las categorías de riesgo inaceptable fueron **ampliamente aceptadas**, pero con la conciencia de que Chile deberá actualizar esta lista conforme evolucione la IA y su impacto social (lo cual se podrá hacer vía ley o quizás aprovechando la revisión de la norma en unos años).

* **Delegación de la Lista de IA de Alto Riesgo a Reglamento:** Algunos parlamentarios manifestaron inquietud porque la **identificación de sistemas de alto riesgo** quede entregada a un reglamento del Ejecutivo. Si bien la ley fija criterios generales, la definición caso a caso – p.ej. si una IA para diagnóstico médico específico es alto riesgo o no – se haría vía reglamentaria con consejo técnico. Esto fue justificado en la necesidad de **flexibilidad técnica**: el ecosistema IA cambia rápido, y listar cada caso en la ley sería engorroso y rígido. No obstante, se planteó como contrapartida la importancia de control legislativo: que el Parlamento supervise la elaboración de ese listado y sus actualizaciones. La integración de expertos mediante el Consejo Asesor de IA brinda cierta legitimidad técnica, pero la crítica es que en última instancia será un acto administrativo. La fórmula adoptada parece razonable dado el contexto – muchos países siguen ese camino – pero será fundamental la **transparencia y participación** en la elaboración del reglamento. De hecho, el proyecto confirmó la vigencia del rol del Consejo para la Transparencia en este punto, asegurando que los criterios y listados de IA alto riesgo sean información pública accesible, sujeta a escrutinio ciudadano.

* **Enfoque en Usos vs. Tecnologías:** Un aspecto conceptual debatido fue si la ley debe regular **tecnologías de IA per se o sus usos**. Finalmente se consagró el criterio basado en *usos específicos*, lo que es técnicamente más sólido (porque un mismo algoritmo puede ser benigno o riesgoso según cómo se use). No obstante, esto implica que **responsabilidad y cumplimiento recae en operadores** más que en desarrolladores originales. Algunos desarrolladores advirtieron que podía resultar complejo para un proveedor saber todos los usos finales de su IA. La ley mitiga esto definiendo claramente los roles (proveedor, implementador, etc.). Si un desarrollador pone en mercado una IA general, y un tercero implementa un uso de alto riesgo, este implementador será el principal obligado a cumplir reglas (aunque el proveedor original también tiene deberes si comercializa en Chile). Este enfoque basado en usos fue considerado **acertado** y necesario, pero requerirá mucha **difusión y guía práctica** para que cada actor entienda sus obligaciones en la cadena de suministro de IA.

* **Sistemas de IA de Uso General (Foundation Models):** Ligado a lo anterior, un tema emergente fue cómo abordar los **modelos de IA de propósito general** (p.ej. GPT, DALL-E, etc.), que no se diseñan para una finalidad específica pero pueden emplearse en contextos diversos, incluso de alto riesgo. El proyecto no los excluye: al contrario, mediante indicaciones se dejó claro que *deberán cumplir siempre con las obligaciones de art. 8*. Esto implica que un **modelo general** (por ejemplo, un gran modelo de lenguaje) deberá incorporar por diseño gestión de riesgos, transparencia, etc., incluso si en principio no está destinado a un uso crítico conocido. Esta exigencia es **vanguardista** y alinea a Chile con la tendencia regulatoria de la UE (que añadirá obligaciones para “IA de propósito general” en su Acta). Sin embargo, es un área compleja: las empresas que desarrollan estos modelos (a menudo extranjeras) tendrían que adaptar sus prácticas si sus outputs se usan en Chile. La efectividad real de esta provisión dependerá de la **cooperación internacional** y de cómo se interprete en la regulación secundaria. Es un punto a vigilar, pues algunos actores de la industria podrían argumentar dificultad de cumplir reglas de transparencia profundas en modelos tan complejos. No obstante, la intención del legislador es cerrar la brecha por donde se podría eludir la ley bajo el pretexto de ser “tecnología genérica”.

* **Régimen de Sanciones – Proporcionalidad y Efectividad:** Como en toda ley que establece multas altas, hubo discusión sobre **cómo garantizar la proporcionalidad** de las sanciones y su aplicación justa. Con topes de hasta 20.000 UTM para infracciones muy graves, la ley busca un efecto disuasivo fuerte. Esto la equipara con normas como la GDPR europea (que impone multas cuantiosas por violaciones). La preocupación es evitar casos en que pequeñas startups puedan verse asfixiadas por sanciones desproporcionadas, o que la APDP tenga excesiva discrecionalidad. En Comisión, se puso énfasis en que se apliquen criterios de **gradación** y se consideren las características del infractor al imponer la multa. Las indicaciones reforzaron la posibilidad de estándares diferenciados para PYMEs en el cumplimiento, lo que indirectamente también influye en que las sanciones puedan ser menores si la expectativa de cumplimiento era menor. Además, en las normas sancionatorias se suelen incluir atenuantes como la buena fe, subsanación voluntaria del incumplimiento, reincidencia, beneficio económico obtenido, capacidad económica, etc. Es clave que el **reglamento procedimental sancionatorio** (a dictar por la APDP) incorpore estos criterios para guiar la imposición de multas. Otro ángulo: se discutió si las multas debían expresarse en **UF en vez de UTM** para no perder valor con el tiempo, dado que las UTM se reajustan por IPC mensualmente, pero las UF por inflación diaria (en la práctica ambas se reajustan, solo que la UF está indexada al día). Finalmente, mantuvieron UTM probablemente para equiparar con otras leyes sectoriales. En resumen, el régimen sancionatorio se considera **riguroso pero necesario** para asegurar cumplimiento (especialmente de grandes empresas tecnológicas), siempre que se administre con proporcionalidad y se revise su efectividad en la práctica.

* **Rol de las Instituciones y Capacidad de Cumplimiento:** Algunos observadores manifestaron duda sobre si la **APDP tendrá la capacidad técnica y recursos** para ejercer este nuevo rol fiscalizador en IA, más allá de la protección de datos. La APDP (Agencia de Protección de Datos) es un órgano nuevo que deberá formarse y sumar personal especializado en IA. El proyecto contempla esta carga adicional en su Informe Financiero (de hecho, envió la 4ª transitoria a Hacienda por el costo). Es fundamental que se asignen fondos suficientes para dotar a la APDP de equipos técnicos (ingenieros, data scientists, etc.) que puedan auditar algoritmos y verificar cumplimiento. En Comisión se discutió que sin recursos, la APDP difícilmente podría fiscalizar los algoritmos opacos de multinacionales. La respuesta fue que la ley proveerá herramientas, y se confía en cooperación internacional y en el propio efecto reputacional para lograr cumplimiento. Aún así, este es un **desafío crítico**: la mejor ley será letra muerta si la autoridad no puede hacerla cumplir. Por tanto, será crucial la **implementación efectiva**, la capacitación de los funcionarios y la colaboración con entes como la ANCI (para temas de ciberseguridad) y con redes globales de autoridades digitales.

* **Propiedad Intelectual vs. Innovación:** La introducción de una excepción de **minería de datos en propiedad intelectual** tuvo aceptación general por el ecosistema innovador, pero preocupación por parte de titulares de derechos (editoriales, sociedades de autor). La Ministra de las Culturas participó en la Comisión, señal de que este tema se analizó intersectorialmente. El texto propuesto habilita análisis de grandes conjuntos de obras protegidas siempre que no constituya explotación encubierta. Algunos parlamentarios preguntaron cómo se evitará que las empresas tecnológicas aprovechen esta excepción para entrenar IA con contenido protegido sin remunerar a los creadores. La respuesta radica en la frase “no constituya explotación encubierta”: es decir, si se intenta publicar o lucrar con las obras generadas usando piezas de otras obras, no estaría cubierto por la excepción. Queda a interpretación, sí, y ese es el talón de Aquiles. Este punto es **sensible** para industrias creativas (música, literatura, arte visual) ante la IA generativa. Es probable que surjan debates futuros o incluso jurisprudencia aclarando los límites de esta excepción. Por ahora, la evaluación es que la norma equilibra la necesidad de **acceso a datos para IA** con la protección del derecho de autor, en línea con lo hecho en la UE (excepción de *text and data mining*), lo cual es positivo para la ciencia de datos en Chile.

En conclusión, los puntos controversiales giran en torno a cómo **balancear la innovación y la protección**: qué usos prohibir o permitir bajo excepción, cuánta carga imponer a desarrolladores y usuarios de IA, cómo distribuir funciones entre instituciones y asegurar su capacidad, y cómo sancionar sin ahogar la naciente industria local. El proyecto ha abordado muchas de estas tensiones con ajustes finos (ej. excepciones acotadas, diferenciación para PYMEs, inclusión de consejo experto). No obstante, persiste el consenso en que la **clave será la aplicación práctica**. La norma crea un paraguas amplio; quedará en manos del reglamento, las orientaciones del Consejo Asesor, y el buen hacer de la APDP y demás entes el que se alcance el objetivo: **impulsar un ecosistema de IA responsable, transparente y centrado en el ser humano** en Chile.

## 5. Participación Institucional en el Proceso y Gobernanza de la IA

Diversas **instituciones** han tenido un rol activo en la gestación y futuro cumplimiento de esta ley:

* **Ministerio de Ciencia, Tecnología, Conocimiento e Innovación (MCTCI):** Es la **cartera líder** detrás del proyecto. El Mensaje presidencial fue ingresado a través del Ministerio Secretaría General de la Presidencia, pero con sustento técnico del MCTCI, encabezado por la ministra Aisén Etcheverry, quien asistió a las sesiones de la Comisión de Futuro para defender la iniciativa. MCTCI será además la **autoridad regulatoria primaria** una vez vigente la ley: deberá dictar el **reglamento** que establecerá la lista de sistemas de alto riesgo y detalles de cumplimiento, emitirá orientaciones técnicas a los operadores, y presidirá el **Consejo Asesor de IA** (el ministro o ministra CTCI es quien convoca y recibe las propuestas del Consejo). También, MCTCI jugará un papel en las **medidas de fomento** (por ejemplo, habilitar sandboxes sectoriales en coordinación con otros ministerios). La activa participación de MCTCI asegura que la política de IA esté alineada con la **Estrategia Nacional de IA** (Chile lanzó una estrategia en 2021) y que el diseño regulatorio se integre a la institucionalidad científica y de innovación existente.

* **Honorable Cámara de Diputadas y Diputados:** La Cámara Baja es la **corporación de origen** del proyecto, donde se ha desarrollado la primera fase del trámite. En particular, la **Comisión de Futuro, Ciencia, Tecnología, Conocimiento e Innovación** de la Cámara jugó un rol central: no sólo tramitó este proyecto formalmente, sino que ya en 2023 venía estudiando la temática de IA con el proyecto 15.869-19. Esto permitió que la discusión partiera con un *nivel técnico alto*. La Comisión recibió en audiencia a numerosos expertos (académicos de universidades – p. ej. U. de Chile, U. Adolfo Ibáñez, U. de Valparaíso, etc. – representantes de empresas tecnológicas, sociedad civil como Datos Protegidos, Derechos Digitales, etc., y organismos públicos). El **Informe de Comisión** refleja esas contribuciones a través de constancias y minutas. Además, la Cámara consultó formalmente a la **Corte Suprema** sobre aspectos del proyecto que tocaban competencias judiciales, mostrando una coordinación inter-poderes poco usual pero muy sana en aras de prevenir vicios. La Sala de la Cámara aprobó el proyecto en general el 4 de agosto de 2025 con 63 votos a favor (sin votos en contra), lo cual evidencia un **apoyo transversal**. Varios diputados de diferentes bancadas participaron activamente (Tomás Lagomarsino, Eric Aedo, Johannes Kaiser, Gael Yeomans, entre otros, según consta en las votaciones e indicaciones), presentando indicaciones y aportes. Es destacable que el **diputado informante** designado fue Tomás Lagomarsino, quien además fue coautor de la moción original 15.869-19, simbolizando la continuidad del trabajo legislativo.

* **Excma. Corte Suprema:** El máximo tribunal fue **consultado por la Cámara** respecto de ciertas disposiciones (inicialmente el art. 27, luego numerado 28, y conexos) que podrían tener carácter de ley orgánica constitucional al incidir en la administración de justicia. La Corte Suprema respondió mediante Oficio Nº199/24 del 21 de junio de 2024. En su informe, la Corte no se limitó a los artículos consultados, sino que comentó también los artículos 28 (actual 29) y 29 (actual 30), relativos al procedimiento civil de responsabilidad. La opinión de la Corte fue valiosa para ajustar el proyecto: señaló, por ejemplo, que las reglas especiales de competencia para demandas civiles por daños de IA implicaban modificar atribuciones de tribunales (materia de ley orgánica). También advirtió sobre el quorum especial requerido para dichas normas (art. 77 CPR). Esto permitió a la Comisión enmendar o segregar esas disposiciones para evitar problemas constitucionales (aun así, en sala art. 28 fue rechazado al no alcanzar quorum especial). La Corte además pudo haber sugerido mejoras, como aclarar la coordinación APDP-ANCI (dado que la ley 21.663 de ciberseguridad también otorga competencias a la ANCI en incidentes que afecten infraestructura crítica, incluyendo sistemas de información del Estado). De hecho, las indicaciones posteriores incluyen esa coordinación. La **participación de la Corte Suprema** en este trámite refleja la importancia de la materia y ha contribuido a dotar de mayor solidez jurídica al texto, especialmente en lo tocante a debido proceso y competencias judiciales.

* **Consejo Asesor Técnico de IA:** Si bien este Consejo aún no existe (se creará una vez promulgada la ley), es un actor institucional previsto de gran relevancia. Durante la tramitación, la idea de su composición y funciones fue moldeada con aportes del **Consejo para la Transparencia**, la **Sociedad Civil** y el propio Ministerio. Por ejemplo, se decidió **no otorgarle carácter vinculante** para no requerir quorum supramayoritario (ley orgánica) en su creación, manteniéndolo como órgano consultivo. También se especificaron criterios de designación que aseguren idoneidad técnica (incluso se discutió la inclusión de expertos académicos, sector privado, etc., con equilibrio de género). La **Comisión de Futuro** aportó experiencia en este punto, dado que existe un Consejo similar en materia de bioética. En el debate, algunos sugirieron que este Consejo debería más bien depender de un organismo autónomo o de la Presidencia, pero finalmente se radicó en Ciencia (MCTCI) por ser la agencia especializada. Con las indicaciones de agosto 2025, se robusteció su mandato incorporando alfabetización pública, y asesoría multisectorial. Se espera que una vez operativo, este Consejo actúe como **espacio de colaboración público-privada-académica**, emitiendo recomendaciones técnicas que mantengan la regulación actualizada frente a nuevos desafíos de la IA.

* **Agencia de Protección de Datos Personales (APDP):** La APDP es pieza central de la gobernanza propuesta, encargada de la **fiscalización y aplicación sancionatoria** de la ley. No obstante, a la fecha de agosto 2025, esta Agencia **aún no entra en funcionamiento**, pues depende de la aprobación e implementación de la nueva Ley de Protección de Datos (boletín 11144-07), recientemente despachada. Durante la tramitación, la futura APDP estuvo representada por la **Unidad de Protección de Datos del Ministerio de Economía** (que ha sido la antecesora en preparar su instalación). La APDP ha sido dotada en esta ley de amplias facultades, pero también de **responsabilidades**: por ejemplo, debe recibir y gestionar los reportes de incidentes graves de IA, coordinarse con ANCI para incidentes de ciberseguridad, instruir procedimientos sancionatorios complejos, etc. La colaboración de la APDP con la ley de IA fue un punto insistente – tanto que se la nombra en varios artículos (p.ej. art. 4 inc. final, art. 8 final, art. 13, art. 14-15, art. 24-27). En particular, se recalca que la APDP tendrá que emitir **orientaciones a los operadores** en sus esferas de competencia para guiarlos en el cumplimiento de la ley. Ello requerirá que la APDP elabore guías o recomendaciones técnicas (por ejemplo, sobre cómo documentar un sistema de IA, cómo realizar evaluaciones de impacto algorítmico, etc.). La **Dirección de Presupuestos (Dipres)** calculó en su Informe Financiero Complementario los gastos asociados a estas nuevas funciones (personal adicional, sistemas informáticos, etc.). La presencia de la APDP en el diseño legal fue en general bien vista, al ser un organismo especializado en protección de derechos fundamentales (privacidad) que se ampliará a IA; pero también hubo llamados de atención: en Comisión se mencionó la importancia de que la APDP **no absorba competencias de otros organismos**. De ahí la aclaración de que no afecta al Consejo de Transparencia, y la coordinación con ANCI en seguridad. En suma, la APDP es **columna vertebral** de la supervisión en IA, y su efectiva puesta en marcha con recursos adecuados es vital para el éxito de la ley.

* **Consejo para la Transparencia:** Aunque no aparece en el proyecto original, esta institución que vela por la transparencia del sector público se involucró para asegurar que la nueva regulación de IA no disminuya las obligaciones de transparencia activa ni el derecho de acceso a la información. Gracias a su intervención, en las indicaciones finales se **ratificó expresamente su competencia** en materia de acceso a información pública sobre sistemas de IA. Esto significa que, por ejemplo, si un ministerio implementa un sistema de IA de alto riesgo, no podrá alegar secreto absoluto bajo la ley de IA, sino que seguirá sujeto al escrutinio vía Ley de Transparencia (salvo excepciones de seguridad nacional u otras ya contempladas en esa normativa). El Consejo para la Transparencia seguramente aportó desde su experiencia en gobierno abierto, enfatizando la necesidad de **algoritmos transparentes en el Estado**. De hecho, este tema ha sido discutido globalmente: cómo se garantiza que las decisiones automatizadas en entes públicos sean explicables al ciudadano. Con su involucramiento, Chile apunta a mantener consistencia entre ambas legislaciones.

* **Agencia Nacional de Ciberseguridad (ANCI):** La ANCI es un nuevo órgano proyectado en la Ley Marco de Ciberseguridad (21.663, publicada en 2022), responsable de la seguridad de los sistemas críticos. Dado que muchos sistemas de IA pueden formar parte de servicios críticos o procesar datos sensibles, la ANCI fue llamada a jugar un papel complementario. En las indicaciones de 2025 se integró a la ANCI tanto en la fase preventiva (orientaciones conjuntas con APDP), como en la de gestión de incidentes y sanciones (coordinación APDP-ANCI ante incidentes graves y en fiscalización de operadores críticos). Es destacable esta **visión holística**: protege los datos y privacidad (APDP) y la continuidad/seguridad técnica (ANCI). En la práctica, esto implicará que si, por ejemplo, un hospital (infraestructura crítica de salud) usa IA y sufre un fallo de ciberseguridad con fuga de datos, la APDP investigará la violación de la ley de IA/datos y la ANCI asistirá en la respuesta técnica y evaluará medidas para evitar futuras intrusiones. Durante el debate, representantes del Ministerio del Interior o Defensa (a cargo de ciberseguridad) probablemente colaboraron para insertar estos puentes. De este modo, la implementación de la ley de IA quedará **entrelazada con la estrategia nacional de ciberseguridad** – algo muy pertinente considerando que la **seguridad** es uno de los principios rectores de la IA (seguridad desde el diseño) y que las **amenazas cibernéticas** pueden comprometer seriamente sistemas de IA (ataques adversariales, manipulación de datos, etc.).

* **Otros Actores Institucionales:** Además de los mencionados, han participado en el proceso **ministerios sectoriales**. Por ejemplo, el **Ministerio de Economía** (que tiene la Unidad de Datos Personales y está involucrado en transformación digital), el **Ministerio de Justicia** (por la arista de derechos fundamentales, protección al consumidor, y la ley de datos personales), el **Ministerio Secretaría General de Gobierno (Segpres)** en la coordinación legislativa, y el **Ministerio de Hacienda** (que vio la disposición transitoria de gasto). También el **Ministerio de las Culturas, Artes y Patrimonio**, cuya ministra Carolina Arredondo asistió a la Comisión, dado el interés en la excepción de propiedad intelectual y en el impacto de IA en industrias creativas. Igualmente, instituciones como **SERNAC** (protección al consumidor) pudieron incidir – de hecho, la inclusión del principio de protección de derechos de consumidores en el art. 4 se vincula con su mandato. Académicamente, la **Universidad de Chile, la Universidad Adolfo Ibáñez, Universidad de Valparaíso, entre otras**, presentaron a expertos (por ejemplo, se menciona al señor Felipe Tobar, director de Data e IA de la U. de Chile, en la lista de invitados). La **sociedad civil** estuvo representada por organizaciones como Derechos Digitales, Fundación Datos Protegidos, etc., que aportaron la perspectiva de derechos humanos y civiles en la IA. Finalmente, la **empresa privada** (Microsoft, IBM, Google, startups locales) también participó en audiencias, asegurándose de expresar sus puntos (por ejemplo, sobre promover innovación y cuidado con sobrecarga regulatoria).

En síntesis, la elaboración de esta ley ha sido un **ejercicio multisectorial**. Instituciones del Poder Ejecutivo (MCTCI, Economía, Interior, Hacienda, Cultura), del Legislativo (Comisión de Futuro y otras comisiones como Hacienda, e incluso el Senado en observación temprana), el Poder Judicial (Corte Suprema), entes autónomos en formación (APDP, ANCI) y órganos ya establecidos (Consejo Transparencia, Sernac) han **convergido** para dar forma a una gobernanza de IA coordinada. Esto es muy valioso, pues la IA es transversal por naturaleza: su regulación requiere romper los *silos* institucionales. Chile, a través de este proyecto, está sentando las bases de una **estructura institucional colaborativa** donde cada organismo aporta desde su especialidad – Ciencia lidera, Datos Personales fiscaliza derechos, Ciberseguridad protege infraestructura, Transparencia garantiza acceso público, etc. De mantenerse este espíritu de cooperación en la implementación, aumentan las probabilidades de éxito de la normativa.

## 6. Cronología Legislativa Relevante (hasta agosto de 2025)

A continuación se presenta una **línea de tiempo** con los hitos más destacados del proceso legislativo de esta ley de IA en Chile, desde sus antecedentes hasta agosto de 2025, basados en los documentos aportados:

* **24 de abril de 2023:** Un grupo de diputados y diputadas de diversas bancadas (Lagomarsino, Aedo, Medina, Mellado, Olivera, Oyarzo, Santibáñez, Venegas) presenta una **moción parlamentaria** para regular la inteligencia artificial, robótica y tecnologías conexas (Boletín 15.869-19). La moción ingresa a tramitación en la Cámara de Diputados y es asignada a la Comisión de Futuro, CTCI.

* **Año 2023:** La **Comisión de Futuro, Ciencia y Tecnología** realiza un extenso **trabajo prelegislativo** en torno a la IA, tomando como base la moción 15.869-19. Durante ese año se llevan a cabo sesiones informativas, con participación de expertos académicos, organismos públicos y privados, para dimensionar los desafíos de regular IA. Este trabajo sirve de insumo inicial y genera consenso sobre la necesidad de un marco legal flexible y prospectivo.

* **7 de mayo de 2024:** El **Presidente de la República** envía su **Mensaje N°063-372** a la Cámara de Diputados, presentando un **proyecto de ley sobre Inteligencia Artificial** (Boletín 16.821-19). Este proyecto del Ejecutivo recoge parte del contenido de la moción previa, pero enfocándose exclusivamente en los sistemas de IA (ya no menciona robótica explícitamente) y proponiendo la estructura basada en riesgos. Se le otorga **urgencia “suma”** (discusión en 15 días por cámara) para acelerar su tramitación.

* **8 de mayo de 2024:** En la sesión 27ª de la Cámara de Diputados, el proyecto es formalmente dado a conocer. La Mesa de la Cámara dispone enviar copia del proyecto a la **Corte Suprema** para que se pronuncie sobre las normas que pudieran afectar atribuciones judiciales, en particular el entonces artículo 27 (sobre acción civil). Asimismo, por contener una norma (4ª transitoria) con impacto presupuestario, se ordena que sea conocida también por la **Comisión de Hacienda** más adelante.

* **Mayo – junio 2024:** La **Corte Suprema** analiza el proyecto y el **21 de junio de 2024** remite su informe mediante Oficio Nº199/24 a la Cámara. En paralelo, la **Comisión de Futuro** inicia la discusión general del proyecto refundido 15.869-19 y 16.821-19. Asisten la Ministra de Ciencia (Aisén Etcheverry) y su equipo, la Ministra de las Culturas (Carolina Arredondo), y se reitera la convocatoria a expertos que ya habían contribuido en 2023. Se analiza el mensaje y se identifican puntos a mejorar (los antecedentes comparados, definiciones, rol institucional, etc.).

* **8 de mayo de 2024:** (Cronología interna) La Cámara califica de **quorum orgánico constitucional** los artículos 28, 29 y 30 del proyecto, por incidir en organización de tribunales (art. 77 CPR), anticipando la necesidad de 89 votos para su aprobación. También nota que el artículo 13 (Consejo Asesor) *no* es orgánico constitucional al ser un órgano meramente consultivo.

* **3 de julio de 2024:** (Fecha tentativa, no explícita en docs pero probable) La Comisión de Futuro vota la **idea de legislar** (aprobación en general) del proyecto refundido. Según consta, el proyecto fue **aprobado por unanimidad (11-0)** en general, demostrando acuerdo transversal en los objetivos. Con ello, se abre el plazo para indicaciones en particular.

* **18 de octubre de 2024:** El Ejecutivo, a través del Ministerio Segpres y con firma del Presidente, ingresa la **Primera Ronda de Indicaciones** (Nº 234-372) al proyecto. Estas indicaciones, fechadas ese día en Santiago, incluyen al menos dos modificaciones puntuales: obligación para IA de uso general de seguir reglas de art.8, y aclaración sobre sandboxes. Posiblemente también hubo otros ajustes menores en numeración o redacción. (Cabe señalar que aunque el documento de indicaciones del 18/10/24 provisto parece breve, pudo haber más puntos; pero en lo esencial son enmiendas de precisión.)

* **Octubre – diciembre 2024:** La Comisión de Futuro realiza la **discusión en particular** artículo por artículo, incorporando las indicaciones recibidas. Se votan cada una de las propuestas (tanto las del Ejecutivo como las presentadas por diputadas/os). En este proceso, algunas indicaciones son rechazadas, otras aprobadas. Por ejemplo, se indica que la **Indicación 65 del Ejecutivo** – probablemente relacionada con la responsabilidad civil (art. 27 actual 28) – fue considerada de quorum especial y sujeta a votación separada. Muchas votaciones divididas quedan registradas en el informe (como las listadas en las páginas del informe de comisión). Finalmente, la Comisión acuerda un **texto aprobado en particular** para emitir su **Primer Informe**.

* **21 de diciembre de 2024:** (Fecha aproximada) La Comisión de Futuro aprueba el proyecto en particular con modificaciones, y firma el **Informe de Comisión**. Este *Primer Informe*, titulado “Informe de la Comisión... acerca del proyecto de ley que regula los sistemas de IA (boletines refundidos 16821-19 y 15869-19)” está firmado electrónicamente (código de verificación 310F79FA3911B3F1) y fue ingresado a la Sala. En él se detalla el articulado resultante y cómo votó cada miembro en cada indicación, así como las fundamentaciones.

* **8 de enero de 2025:** Según el cronograma esperado de la urgencia, la Cámara podría haber tenido en tabla el proyecto para votación general y particular, pero es probable que la urgencia se haya prorrogado o rebajado en algún momento, dado que recién llegó a votarse en sala en agosto. En todo caso, a inicios de 2025 el proyecto seguía en primer trámite.

* **4 de marzo de 2025:** El Ejecutivo presenta la **Segunda Ronda de Indicaciones** (Nº 339-372), un paquete mucho más amplio de modificaciones, concordado con los aportes de la Comisión y otros stakeholders. Estas indicaciones, de fecha 04/03/2025, reestructuran varios artículos: añaden el principio de explicabilidad, reescriben por completo el art. 6 (usos prohibidos), ajustan definiciones de art. 7 (alto riesgo), detallan las obligaciones de art. 8 (apartados a) a g) reemplazados), fusionan art.9 en 8, renumeran artículos posteriores, introducen coordinación APDP-ANCI, etc. Estas indicaciones son discutidas y votadas en la Comisión en sesiones de marzo e inicios de abril.

* **Abril – mayo 2025:** La Comisión de Futuro integra las segundas indicaciones del Ejecutivo. Algunas pueden haber sido rechazadas o modificadas por la Comisión, pero dado el alineamiento con la Comisión, es probable que la mayoría fueran aceptadas, tal vez con enmiendas consensuadas. Para fines de mayo 2025, la Comisión tendría ya concordado el texto definitivo en particular (tras indicaciones), listo para ser informado.

* **Junio 2025:** Es posible que el proyecto sufriera atrasos por la agenda legislativa (p.ej. priorización de otras leyes). Sin embargo, al ser de iniciativa gubernamental con urgencia, se retoma su impulso antes del receso de invierno parlamentario. El Ejecutivo podría haber reiterado la urgencia suma en junio para acelerar la votación en sala.

* **4 de agosto de 2025:** **Pleno de la Cámara de Diputados** considera el proyecto de ley en su discusión general. Conforme al oficio del Secretario en el expediente, ese día la Cámara **aprobó en general** el proyecto refundido 15.869-19 y 16.821-19, con **63 votos a favor** (y 0 en contra, se infiere). Sin embargo, como se trataba de una ley con quorums especiales, la votación general separó aquello que requería 4/7 u orgánica. Se destaca que el artículo 28 (actual, sobre responsabilidad civil y competencia) fue **rechazado en general** al no alcanzar el quorum de 4/7 exigido. Esto significa que ese artículo tendrá que revisarse para eventualmente incorporarlo vía enmienda con los votos necesarios o reponerlo en segundo trámite. Aparte de eso, el proyecto pasó el corte general. Durante la misma sesión, se presentó la **indicación de la diputada Mónica Arce** sobre incluir una referencia a la ley de niñez en la definición de incidente grave. Dado que esta indicación no fue conocida por la Comisión previamente, el Reglamento de la Cámara exige remitirla a la Comisión para un **segundo informe**. La Mesa ofició al Presidente de la Comisión de Futuro remitiendo todos los antecedentes y la indicación Arce para ese efecto.

* **Agosto 2025:** Tras la votación general, el Ejecutivo detecta la conveniencia de introducir ajustes antes de la votación en particular en Sala (para mejorar el proyecto y aumentar probabilidades de aprobación unánime). Así, el **19 de agosto de 2025** ingresa la **Tercera Ronda de Indicaciones** del Ejecutivo (Nº 174-373), dirigidas a la Comisión de Futuro para su segundo informe. Estas indicaciones, resumidas en un Informe Financiero Complementario, incluyen aclaraciones de definiciones, objeto centrado en usos, competencias del Consejo de Transparencia, lista de riesgo inaceptable y alto riesgo refinada, nuevas funciones al Consejo IA, facilitación de datos en sandboxes, y promoción de alfabetización en IA. Llegan justo a tiempo para que la Comisión las analice junto con la indicación parlamentaria pendiente.

* **Fines de agosto de 2025:** La Comisión de Futuro se reúne para elaborar el **Segundo Informe**. En esa instancia, debe pronunciarse sobre: la indicación de la diputada Arce al artículo 3, las indicaciones ejecutivas 174-373, y posiblemente resolver qué hacer con el artículo 28 rechazado (podría suprimirse o modificarse para intentar su aprobación posterior). Es probable que la Comisión haya aprobado incorporar gran parte de estas indicaciones, dado que afinan el texto sin cambiar sustancialmente el consenso. El segundo informe probablemente quedó listo a inicios de septiembre 2025.

*(Cabe mencionar que el detalle de este segundo informe no está en los documentos, pues se extiende hasta la fecha límite pedida – agosto 2025. Pero este sería el curso lógico.)*

* **Próximos pasos (post-agosto 2025):** Si bien excede el marco temporal solicitado, para contextualizar: tras el segundo informe, la Cámara deberá votar el proyecto **en particular** artículo por artículo. Con el respaldo transversal que ha mostrado, es esperable su aprobación casi total. De haber artículos de quorum pendientes (como el 28 de responsabilidad civil), podrían votarse separadamente con quorum especial; si no obtienen suficiente apoyo, se eliminarán o ajustarán. Luego el proyecto pasará al **Senado** en segundo trámite, donde iniciará probablemente en la Comisión de Desafíos del Futuro o en la de Constitución, según se determine. El Senado podría introducir cambios adicionales, aunque con la base ya trabajada es posible que avance rápido. Finalmente, de aprobarse en el Senado, sería ley de la República, entrando en vigencia en la forma dispuesta (posiblemente con vacatio de meses para preparar reglamentos y la operación de APDP y Consejo IA).

En conclusión, **hasta agosto de 2025** el proyecto de ley de IA en Chile ha recorrido con éxito su primer trámite constitucional en la Cámara de Diputados, enriquecido por numerosas indicaciones y aportes institucionales. Ha habido un proceso legislativo riguroso, participativo y técnicamente informado, lo que augura una ley sólida. Resta aún el paso por el Senado, pero Chile podría convertirse prontamente en uno de los primeros países de América Latina en contar con una **ley marco de inteligencia artificial**, anticipándose a los desafíos y oportunidades que esta tecnología conlleva.

**Fuentes:**

* Mensaje Presidencial, Boletín 16821-19, 7 de mayo de 2024.

* Informe de Comisión de Futuro CTCI, Boletines refundidos 15869-19 y 16821-19, 2024.

* Indicaciones del Ejecutivo 18-10-2024 (Nº 234-372).

* Indicaciones del Ejecutivo 04-03-2025 (Nº 339-372).

* Informe Financiero Complementario – Indicaciones 19-08-2025 (Nº 174-373).

* Oficio de la Cámara al Presidente de Comisión, 4 de agosto de 2025.

* Texto del proyecto de ley (articulado consolidado en Comisión).

* Debates en Comisión (votaciones de indicaciones, intervenciones).
