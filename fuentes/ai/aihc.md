# The Design of Human-Centered Artificial Intelligence for the Workplace

## Introduction

### The Design Space of Human-Centered Al Solutions

**Abstract**  
The rapid advancement and adoption of Al technologies, including large language models (LLMs) and chatbots, marks a new era in workplace and enterprise support by enabling the generation of human-like artifacts. Through a broad range of machine learning (ML) algorithms and deep neural networks, Al enables mission-critical functions such as pattern recognition, decision-making, and automation. To navigate this exciting albeit complex technological landscape, we introduce a human-centered AI (HCAI) solution map, which classifies AI solutions according to two dimensions: human-centered Al purpose, i.e., whether the Al solution is primarily intended to assist, augment, automate, or replace the human expert, and human-centered Al cognition, i.e., the type of cognitive function support such as sensing, decision-making, and action execution. Moreover, the HCAI solution map organizes Al solutions into roles ranging from assistants and tools to orchestration agents and automation agents. These roles vary in the way they support users and reflect different levels of human involvement during the process (i.e., in-the-loop, on-the-loop, or out-of-the-loop). Designing Al systems with human-centered qualities such as empathy, transparency, and explainability fosters positive user experience and trust. Characteristics such as context awareness, human-centeredness, and human-control, in turn, determine the effectiveness of Al solutions in adapting to situational needs and corresponding user interactions. Ultimately, by considering these human-centered dimensions, characteristics, and qualities when designing AI solutions, organizations can align the Al solution with the intended users' needs and empower users and maintain human control while creating value for all stakeholders.

### 1 Human-Centered AI Solutions

With the public launches of chat applications based on large language models (LLMs), we are experiencing global excitement and adoption of artificial intelligence (AI) in enterprise and workplace support-mainly in the form of natural language dialogue interfaces in combination with generative capabilities to produce human-like artifacts such as written text, images, sound, and computer programming code. This unprecedented capacity to generate artifacts, which until today have been considered exclusive to knowledge-based workers with special technical skills, has become the poster child of Al in the press.

However, such LLM-based solutions are only one flavor of Al systems, i.e., advanced technologies that leverage large and diverse data inputs to perceive and respond rationally to the world around them. Machine learning (ML) and Al technology have been in production for several decades in different forms in the consumer and business world. ML algorithms are powerful in classifying and clustering information even predicting critical events. Deep neural networks successfully recognize patterns in visual information or human preferences to extract meaning or make personalized recommendations based on historical data.

As can be seen by these seemingly heterogeneous examples, there is a diverse set of enabling technologies such as ML, deep neural networks, LLM, and problem solvers and a broad spectrum of purposed solutions that are either built to support users, perform functions intended to augment human performance, or aim to establish automated decision-making and execution. This broad spectrum is also reflected by the variety of topics presented in this book's 25 chapters.

To help readers navigate this book, we introduce a solution map for human-centered AI (HCAI) applications (see Fig. 1), which is arranging the different flavors of Al solutions not by the underlying technology rather by the following two human-centered dimensions:

- **Human-Centered Al Purpose:** What role does the Al solution play in relationship to the user? Should the solution assist, augment, orchestrate, or automate?
- **Human-Centered Al Cognition:** What type of cognitive function support is provided by the Al solution to help the user sense information, make decisions, and enact decisions?

This two-dimensional canvas is used to map out popular solution paradigms of human-centered Al systems according to their locality of the provided intelligence and their primary purpose.

### 2 Human-Centered Al Purpose

Each Al solution is explicitly or implicitly designed with a certain purpose in mind. A human-centered purpose defines the relationship between the human and the Al system:

- **Al assistants** directly support users by typically providing natural language interfaces, answering questions, giving recommendations, and/or triggering/executing actions specified by the user. For example, a copilot assists in writing code, choosing between decision options, making the user aware of a risk, and more.
- **Al augmentation tools** provide packaged Al-enabled functionality that augments a user's abilities and/or capacity. Such technology solutions are used exclusively in a direct, interactive manner to accomplish a specific user task. For example, an Al-powered graphics tool may help the user edit images with just a few clicks. The more generative the tool is, the more it serves the role of an assistant that performs task-related actions on behalf of the user upon request.
- **Al orchestration agents** function in support of human-centered purpose or human-centered cognition. In the former, orchestration agents manage delegation and human control. They are the human interface layer for coordinating and supervising Al systems and setting interventions points. In the latter, orchestration agents coordinate across multiple agents in different cognitive realms. For example, in autonomous driving, a situation awareness agent in a moving vehicle senses (i.e., detects) a pedestrian who is anticipated to step on the street; a decision-making agent decides that there is a need to slow the vehicle down; and an execution agent acts by controlling the brakes as needed to stop the vehicle in a safe way.
- **Al automation agents** are Al systems that run decoupled from user interaction to provide specific cognitive function support, including sensing the environment to establish a situation awareness, deciding on what task(s) needs to be carried out, and executing the corresponding task(s). In doing so, they enable automation across the complete Sense-Decide-Act loop. Since automation agents can perform detached from users, they must provide configuration and intervention user interfaces to enable humans to control and align the behavior of these automation agents with their preferences and needs. Human operators delegate a task or cognitive function to either orchestration agents that map a recognized user intent to an available agent (e.g., LLM plug-in sourced by ChatGPT in response to a user input) or automation agents (e.g., a surveillance system that automatically recognizes strangers on a property).

### 3 Human-Centered Al Cognition

By "cognition," we refer to the set of human abilities as described and modeled in cognitive psychology. Every human has foundational mental skills such as perception, working memory, memory recall, and more sophisticated executive cognitive skills such as problem-solving, decision-making, planning, reasoning, and creativity. While LLM systems are impressive in their reasoning and generative capabilities, they lack the concept of fact-based retrieval, decision-making, and foremost perception. This is why they are primarily used as an assistant or generative tool, and not as a decision or automation tool.

Human-centered cognition describes the foundational type of cognitive skill that is provided by Al systems. We focus on three cognitive realms that are required to create end-to-end automation loops:

- **Sense:** The capabilities of an Al system are perceptual or help the user to monitor some task-relevant state of the external environment. This can be an Al-powered fraud detection agent at a bank that discovers an unusual combination of financial transactions, or a call center service that recognizes the risk of customer churn based on interactions with the call center agent. What all these Al solutions have in common is that they run independently of the user to observe, comprehend, and detect a critical event that requires a decision or action.
- **Decide:** Al systems that are specialized in making decisions are traditionally rule-engines that can take an input and make a rule-based decision. For complex problems, domain-specific problem solvers can generate decision options or plans that represent a short list of the best solutions. Those systems depend on input from humans or sensing agents. Complex decisions involve the creation and simulation of decision options and related action plans that are complex mental activities. Users must know typical approaches of how to respond and must be able to anticipate the outcome.
- **Act:** To enact a decision, an automation agent must have the ability to trigger actions or execute actions. Those automation agents are typically integrated with other generative plug-ins or Enterprise Resource Planning (ERP) systems to trigger a specific action (e.g., block financial transactions; generate personalized email; and send it to customers). While generative actions are intrinsically cognitive skills, not all actions are of cognitive nature. However, the planning of multiple actions and the specification of an action requires context knowledge and domain knowledge even after the decision has been made.

### 4 Elasticity and Interoperability of HCAI Agents

The two-dimensional canvas illustrated in Fig. 1 helps to classify Al solution paradigms according to their locality on the dimensions of human-centered purpose and cognition, but also illustrates the interoperability and necessary elasticity on both dimensions. While an assistant may be optimized to understand user preferences and map intents to action, it may still require other automation agents that are specialized on a specific cognitive function and operate in support of any cognitive realm.

A copilot experience may also allow the user to delegate certain functionality to automation agents to reduce mental workload and to define policies for intervention points. In this role, the copilot becomes the tool to meta-manage and oversee delegated and automated cognitive functions provided by Al-based automation agents.

When users directly engage with automation agents, the interaction paradigm extends from using a tool to one of coordinating, collaborating, and sharing information between humans and Al-powered automation agents. The structure of such multi-agent automation networks may either be designed and bounded a priori by business stakeholders, or adapted ad hoc by human agents based on situational needs.

Having an explicit understanding of the to be supported solution paradigms is important as it sets the stage for the design and technical implementation. Each paradigm maps to typical requirements and design patterns and sharpens the identity of each automation agent contributing to a larger solution. Since usage patterns of Al systems vary over time depending on trust and situational needs, the role of an Al solution may shift from assistance to delegation, to automation, and then back to assistant if there is a need for human intervention.

### 5 Characteristics of HCAI Solutions

The human-centered Al solution map presented in Fig. I focuses on the primary purpose and cognitive functions supported by the Al systems. Depending on those two aspects of the Al solution, certain design principles and user experience qualities are likely to be dominant and of paramount importance. Figure 2 puts forth a set of characteristics corresponding to each Al solution type according to its human-centered purpose:

- **Context Awareness:** Each Al system is linked to the external reality relevant in the context in which it operates. The Al system represents this context in the form of user history, time series, behavioral graphs, or dedicated digital twins among other data-driven representations of the external state within the system. Such context representations enable the Al system to establish a specific kind of awareness among user awareness, task awareness, agent awareness, situational awareness, and mission awareness. Each kind of awareness is characterized by attributes, which are included as a non-exhaustive list in Fig. 2.
- **Human-Centeredness:** Human-centered design objectives describe which quality dimensions of an Al solution should be optimized for a given use case to evoke a favorable user experience. When developing design requirements for Al-based solutions, additional qualities such as empathy, trust, explainability, transparency, intervenability, and goal alignment, among others, are important. While these and other Al solution qualities apply to all layers of Fig. 1, a few of them become more prominent depending on the level of human control characterizing the Al solution, as proposed in Fig. 2.
- **Human Control:** Distinguishing Al solutions according to the primary human-centered purpose they support also reflects the level of human control maintained in the process. To what extent do direct or indirect users have control over the use, behavior, and outcome of the Al solution? Is the human at the center of all actions, using Al for consultation? Or are Al systems decoupled from direct user interaction and instruction, becoming increasingly self-sufficient in executing tasks? Depending on how deeply the user remains involved, those usage patterns with Al-based systems are referred to as keeping humans in-the-loop, on-the-loop, or out-of-the-loop. Each level presents its own ergonomic, ethical, and safety challenges.

For an Al solution to be successfully adopted by users, it is important not only to identify the business case but also to clearly define the human-centered role the Al solution should play. The posture of an Al solution should align with its human-centered role. For example, common postures are evolving for copilots, as well as for supervision and user intervention in automation agents. Once mapped to one of these roles, the Al solution's design can focus on supporting the corresponding stakeholder requirements, technical requirements, and data requirements for generating reliable context awareness and human-centered qualities.

## Part I

### Human-Centered AI in Business

#### The Situation Awareness, Decision, and Execution (SADE) Ladder: A Framework for Human-AI Collaborative Decision-Making

**Abstract**  
In order to implement intelligent systems that exhibit autonomous behavior, the system must develop its own situation awareness and ability to execute the decisions being made. To model these end-to-end decision loops, we propose a framework that combines situation awareness, decision-making, and action-planning theories into one decision ladder that helps to understand the requisite anatomy and ergonomics of intelligent systems.

This Situation Awareness and Decision Execution (SADE) Ladder acts as a catalyst bringing together different research disciplines specialized in each of the three referenced domains. This synergistic view across multiple research agendas manifests in a conceptual framework to understand better the interaction dynamics in human-Al interactions and the importance of transparency and explainability in multi-agent networks to help the human experts understand the locality of artificial system intelligence and the underlying rationale of system-generated decisions and action plans.

Identifying the different cognitive demands and sub-functions within the SADE Ladder enables practitioners to derive the appropriate requirements from cognitive work analysis and design intelligent systems for workplaces that are trusted and accepted by their users.

### 1 Introduction

Artificial intelligence (Al) systems are used increasingly in workplaces not only to augment capabilities of the human operators and domain experts but also to automate human work and business processes. Unlike traditional automation systems, which support well-defined repetitive work, Al-based automation agents can deal with significant uncertainties in the environment and have sufficient reasoning and problem-solving capabilities to make autonomous decisions about responding to contemporary conditions and initiating actions. Given those higher-level cognitive functions, Al systems are often referred to as intelligent systems.

The level of autonomy of an automation agent depends on its capability to perform an end-to-end decision loop comprised of being aware of the current situation, making decisions, and turning decisions into action without the need for human intervention. While the goal is to deploy such systems to offload work from the human experts and improve scalability and quality of results, it is also important to keep the human in the loop to oversee system performance and avoid deskilling and detachment from operational awareness. Moreover, if direct human input is not needed in the specific context of system use, maintaining the human on the loop assures their ultimate control, oversight, and action override if and when they deem it appropriate.

To better understand the potential of integrating human expertise and Al-based automation systems, we propose a holistic conceptual framework of human/system-agent decision-making, referred to as Situation Awareness, Decision, and Execution (SADE) Ladder. The SADE Ladder benefits designers, developers, implementers, and users of automation intelligent systems by disentangling the prerequisites to unfolding the full potential of distributed cognitive skills and human-Al interactions.

The ultimate goal of process automation is to create end-to-end understand/decide/act control loops that use situation awareness as a basis from which to initiate and inform decisions that trigger actions to reach a desired outcome or state. However, full autonomy, i.e., the human agent out-of-the-loop, is achieved only when one or multiple automation agents support the entirety of these functions. In most cases, human will be on- or in-the-loop and perform a task or process in a more collaborative way-often referred to as human-Al teaming.

Teaming as a verb refers to the fact that all agents that contribute to a task or process must form an ad hoc team and collaborate and coordinate in some form to achieve an intended outcome (Edmondson 2012). An Al system agent supporting some or all of the aspects described in the SADE Ladder will not only be capable of performing tasks in a self-directed autonomous way, but must possess team awareness and collaboration skills to successfully coordinate and communicate across all members of a human agent and autonomous agent team (HAT) (O'Neill et al. 2022).

In this chapter, we introduce the different phases and levels of the SADE Ladder in more detail and refer to research related to each phase. The SADE Ladder not only is helpful to understand the core component of autonomous decisions loops but also affords an understanding of the collaboration dynamics between human and Al systems along the end-to-end decision-making cycle of any automation system that can make decisions and respond to situational needs. Through an understanding of the underlying theoretical concepts, we identify requirements and patterns that must be met to ensure the successful implementation and adoption of autonomous AI systems in the workplace.

### 2 End-to-End Decision-Making Loops

One of the key motivations for automating human tasks is to free the human agent from performing the task and to allow the reallocation of their cognitive and other resources (e.g., time) elsewhere. However, such task reallocation comes with the inherent risk of the human operator over time unlearning the requisite task skills and becoming detached from the task context in general, which can pose a challenge when the human agent is asked or needed to take over control having low situation awareness.

For this reason, much of the research related to cockpit automation (Jones and Endsley 1996; Munir et al. 2022; Politowicz et al. 2021; Salotti and Suhir 2019) and driver assistance (Golestan et al. 2016; McCall and Trivedi 2007; Rockl et al. 2007; Wulf et al. 2014) focuses on the concept of situation awareness and how to ensure that a basic level of situation awareness is shared between human and system agents to sustain the human agent's readiness to intervene if and when necessary.

Situation Awareness (SA) as a concept was initially presented by Endsley (1995). The need for SA in many task domains is self-evident, from aerospace to military planning, where obtaining and maintaining SA is critical to assuring safety or mission success. However, the concept of SA has been applied to many other contexts. Its relevance for the workplace context has been shown when studying the usage patterns of Al systems of real business users (Beringer et al. 2022; Jiang et al. 2022). SA differences were a typical cause for tensions within human-Al teams.

Regarding our interest in autonomous end-to-end decision loops, SA is one important step of the ladder, comprising three levels: perception, comprehension, and projection. In the original conceptualization of SA (Endsley 1995), the decision-making process itself was not considered. Specifically, the actual mechanics of decision-making and the role of human expertise in the decision-making process were neither considered nor included in the original conceptualization of SA.

Furthermore, when considering the addition of Al systems, which are becoming more capable and granted greater autonomy, it would be fair to conclude that an appraisal of more formal decision-making models and how they fit within an SA framework is warranted.

In this regard, early work from Rasmussen (1983) and Parasuraman and Riley (1997) introduced models outlining different decision-making stages to explain the different behaviors and cognitive demands for humans when making decisions at different levels of complexity.

Rasmussen (1983) investigated human-machine control systems and formalized (Fig. 1) how decisions transform inputs (information) into outputs (action). These transformations can be perceived as end-to-end decision loops. Similar to Endsley's work on SA, Rasmussen introduced a formal model of human performance comprising three levels on the basis of conscious planning involved when processing information as inputs for decision-making.

At the lowest level, sensory input is directly transformed into simple actions or skill-based behavior control loops without conscious control. These control loops do not require higher-level mental processes and hence can often be assimilated into autonomic (automated) behaviors. For example, drivers maintaining a specific distance from the car in front of them is a skill-based behavior requiring no conscious planning and is instead a smooth and automated decision-making process. A similarly automated process can be embedded in Al-enabled systems such as self-driving cars, which are often referred to as autonomous vehicles (a misnomer as no vehicles exist to date that are fully autonomous).

At the next higher level of conscious planning, decision-making concerns the application of rules to map an input (condition) to an output (action). This decision-making mode requires rules to be in place and known by the Al agent, allowing a transformation of an input into a known set of actions by applying a rule set. For example, if the temperature reaches a specific level that is deemed a priori to be "too high" by the human, the Al agent would turn on the cooling function of an air conditioner; and conversely, if the temperature reaches a level that is defined a priori as being "too low," it would turn on the heating function of the same air conditioner.

At a higher level of system autonomy, an Al agent may be designed to "learn" over time, for example, by considering the human overrides performed over time in association with turning on the cooling and the heating functions, respectively, and subsequently adjusting the temperature levels associated with the activation of the corresponding function in an ongoing continuous, dynamic process.

The highest level of conscious planning reflects situations of greater uncertainty (e.g., due to incomplete situation information) and increased mental workload during decision-making, as it is either unclear what rules apply or the rule set may not be exhaustive to support decision-making. If these elements are missing and cannot be easily provisioned, then decision-making shifts from rule-based to the highest level-knowledge-based. The agent gathers additional information to perform causal analysis and generates decision options based on experience and technical understanding of the domain. This process can result in a lengthy investigation to understand the problem (situation) and planning exercises to generate action plans with the desired impact.

### 3 The Situation Awareness and Decision Execution Ladder

Inspired by the work on situation awareness (Endsley 1995) and Rasmussen's levels of human-machine control (Rasmussen 1983), and the visual metaphor of a decision ladder, (Rasmussen 1976), to connect upstream sensing with downstream action, we propose the Situation Awareness and Decision Execution (SADE) Ladder framework (see Fig. 2), which integrates the concepts of situation awareness and action planning with an iterative decision-making process to form a model of an end-to-end decision-making loop. The left side of the ladder considers the three levels of Endsley's situation awareness model (i.e., detect, comprehend, project). It introduces a hierarchy of cognitive functions, from the simple observation of facts to comprehension and transformation into meaningful information and further diagnosis to project the future state. These levels also relate to Rasmussen's conscious planning levels, given that direct feedback loops are typically based on simple data inputs that are automatically (or subconsciously by the human) translated into action. In comparison, rule-based decisions are grounded on understanding conditions requiring comprehension and synthesis and interpretation of input data as meaningful information that, in turn, drives action.

Elaborating on the SADE Ladder shown in Fig. 2, Endsley's situation awareness model and Rasmussen's decision ladder metaphor are not perfectly aligned. However, by harnessing the respective strengths of each perspective, an integrative framework can help guide the design of a successful human-Al agent interaction that preserves cognitive heterogeneity across both human and Al agents. The proposed SADE Ladder is composed of a left side concerned with the building of situation awareness, a central pillar concerned with the decision-making process, and a right side concerned with the execution of actions pertinent to a decision.

**Phase 1: Situation Awareness**  
The left side follows Endsley's model of situation awareness: detect is the initial stage involved with identifying or detecting a new fact or event; this is followed by comprehend, where once detected, the next step is to understand, interpret, and synthesize this information to recognize patterns or anomalies; and finally, project involving predictions or forecasting events. These elements align with building situation awareness.

**Phase 2: Decision-making**  
The central pillar consists of discrete, dynamic decision-making processes informed by a decision-maker's situation awareness. The pillar does not comprised levels that are interconnected in a linear manner; instead, each level represents a decision-making "mode," whereby the developing situation awareness of the decision-maker maps to a decision-making mode that, when coupled with prior knowledge or action plans for a given event, can lead to the execution of an action.

The direct control loops relationship represents decisions where incoming information directly leads to an action based on a closed, algorithmic feedback loop. For example, keeping the temperature within a certain range or maintaining the speed of a vehicle requires only low-level detection of temperature or speed and making immediate, necessary adjustments. In this, the results of actions are monitored, and direct feedback is used to proceed with further action executions or to achieve or maintain the intended outcome.

Rule-based decisions are based on recognizing known conditions in the environment that can be mapped to predefined actions. Identifying conditions typically requires comprehension of incoming raw or discreet information after converting it into higher-level, meaningful information; for example, in the case of a self-driving car, the Al system is synthesizing low-level image data into the recognition of an object as a person who is walking and about to cross or crossing the street.

Associative mapping is a heuristic applied by experts who instinctively map fuzzy situations to established action plans based on similarity with past symptom patterns. Similar to neural networks, the exact rationale of such decisions cannot be clearly articulated and is based on experience.

More complex issues may require knowledge-based reasoning utilizing decision-making strategies that require an iterative, focused, problem-centric analysis of the situation in reference to an intended outcome or goal. This knowledge-based decision is modeled as a higher-level strategic loop involving investigation, prediction, and simulation to establish a better understanding of the situation and causal relationships, develop action plans that consider constraints, and compare anticipated outcomes against agreed goals.

**Phase 3: Execution**  
The right side of the SADE Ladder encompasses execution. It includes action planning, where plans are detailed after solutions have been found and decisions have been made, specifying what actions are required, by whom, when, and in what sequence. Informed by the decision-making process and cumulative situation awareness, the action specification details actions, potentially down to the operational level, including concrete resources and precise parameters for implementation.

Action is the actual implementation of a decision or solution; any stage in the decision ladder can lead to action and implementation given sufficient cumulative situation awareness and existing solution frameworks. Examples of actions are sending control commands to a system, initiating a business transaction, or creating an artifact.

### 4 Design Implications of the SADE Ladder

The SADE Ladder is a framework that improves our understanding of the anatomy and characteristics of AI systems that have the potential to perform tasks or entire business processes in an automated manner. Grounded in this framework, we next discuss selected topics related to human-Al interactions and deep-dive into the two areas of decision-making and execution that complement Endsley's situation awareness model.

#### 4.1 Designing with Humans In, Over, or Out of the Loop

The SADE Ladder helps to understand the different levels of automation and enabled degree of system autonomy. Typically, human experts still own the decision-making step, particularly when complex knowledge-based decisions have to be made. In these cases, the human is "in the loop" and directly involved in a hybrid loop, which consists of human experts and Al systems supporting the three phases of the loop. In a hybrid loop, Al is only used selectively to improve situation awareness, generate decision options, simulate scenarios, or plan and perform actions. The human expert may exclusively own one entire phase or may team up with automation agents that augment or scale human cognitive skills to achieve better outcomes. Only if automation agents cover all aspects of a decision-making loop, we will have a fully autonomous system where the human will be "over the loop" (or "on the loop") or "out of the loop." "Over the loop" refers to workflows where the human still provides supervision of the system and tightly monitors the quality of results on an ongoing basis. In contrast, "out of the loop" means the human is detached from concrete work and only manages the Al systems on a meta-level, for example, setting goals and boundaries or facilitating the Al system's learning.

The level of autonomy may be designed upfront at the organizational level, but in reality, the level of autonomy evolves from the trust and acceptance of the human experts in the Al system, which may vary based on the complexity of the task at hand. If rule-based decisions can be made based on information Al-based agents can reliably derive from the external environment, an autonomous rule engine can make the decision that triggers a predefined action.

For example, an Al-powered marketing solution detects an anomaly in a customer's behavior and automatically triggers an email to make a discounted offer to this customer. If a marketing analyst trusts the detection algorithm and is aligned with the decision-making of the rule engine to send the offer, the analysts will be happy to let the system run and make autonomous decisions and actions.

However, there are also situations where human expertise is needed to analyze a complex situation or define high-impact actions. For example, the same Al system may notify the marketing expert that a key account is at risk of moving to another vendor. In this case, the marketing expert would install a task force to analyze the situation further and propose mitigation actions. In this example, the autonomous systems escalated to the human expert and switched from autonomous agent mode into an "interactive tool" mode to support human-driven problem-solving activities and action planning.

Given these changing requirements, depending on the situation's complexity and the human expert's trust in the Al system to produce intended outcomes, an Al-based system must be flexible enough to support different levels of autonomy and the transition between these modes. The question is then not so much if or if not the human should be in the loop, but when. At the highest level, the human is always in control to engage or disengage an autonomous system, which makes the autonomy of a system always a bounded autonomy (Schraagen 2024).

#### 4.2 Designing for Human Expertise

In the SADE Ladder, we model knowledge-based decision-making as its own iterative process aiming to deeper analyze a situation in the context of a specific problem and to create and compare decision options on the execution side as potential responses to a specific problem. The role of Al systems, in this case, is to augment this human-driven decision-making with Al systems that can process massive amounts of data and recognize relationships to inform human decisions, as well as optimizations and simulators to evaluate or recommend solutions.

Al agents employ vast data resources to create customized knowledge bases, enabling comprehensive decision-making for designated use cases. Their capability to swiftly retrain and optimist within specific contexts outpaces human cognition, not only in speed but also in comparative optimization. Nonetheless, the value of human expertise within this cooperative decision-making framework appears undervalued and under-researched, and the loss of expertise is of growing concern (Fügener et al. 2021).

Expertise refers to the knowledge, skills, and experiences an individual has acquired throughout a lifetime or career. There is also a perception, specifically in industry, that the expert is someone who knows the rules, range of procedures, and routines required to perform a given task or decision with high performance (Farrington-Darby and Wilson 2006), but who also knows when to step outside of the bounds of the task to apply novel strategies to novel problems. Expertise is often posited as both static and cumulative, whereby any decisions the expert makes are based upon explicit deliberation (i.e., logical reasoning). However, research has shown that in many cases, experts bypass logical reasoning in favor of a more intuitive process (Naikar 2010), formalized as expertise-based intuition (Salas et al. 2010) and more recently referred to as naturalistic decision-making (Klein 2015).

Naturalistic decision-making (NDM) is rooted in the dual systems thinking theory of human cognition posited by (Kahneman 2011). Dual-process theory differentiates between two cognitive modalities: System 1, representing heuristic-driven, automatic thought, and System 2, denoting methodical, effortful reasoning. Moreover, NDM accentuates the significance of experience, intuition, and situation awareness in complex environments while acknowledging the necessity of analytical deliberation in certain circumstances.

Various approaches have been applied in cognitive work analysis to analyze decision-making processes and inform decision support systems design (Banks et al. 2020). These frameworks, including the SADE Ladder, describe the different demands and sub-functions associated with System 2 decision-making in a normative fashion. To acknowledge the existence of intuitive experience-based decision-making, we added an associative mapping mode in addition to the rule-based mapping mode to indicate that mapping a situation to a response can also be based on a fuzzy association of a situational pattern in the left pillar to an established action pattern in the right pillar. This type of intuitive decision-making is the focus of the Recognition-Primed Decision Model (RPDM). The RPDM describes how experts develop heuristics that resemble a pattern-matching paradigm where situations are mapped to known situations and then to the corresponding procedures associated with the known situation. Instead of rule mapping, RPDM describes how the entire situations are mapped (Naikar 2010).

The RDPM touches on an interesting aspect of the decision ladder, which relates to how the knowledge-based decision process works in ecologically valid contexts. These contexts vary across decision-making domains but may include conditions where situation awareness has a high level of uncertainty because it includes a probabilistic assumption about the future (using projection, prediction, and/or simulation), which transforms the decision-making step into a discussion of influencing and causal factors rather than the application of simple rules.

Other conditions that may arise from ecologically valid decision-making include if the set of facts or insights that built the current situation awareness model is incomplete and requires more focused investigation or diagnostic analysis, if no rule set or model exists that translates a condition into an action, or if the optimization criteria and decision strategy itself become the topic of the discussion or if the impact of an action has an uncertainty of outcome. The SADE Ladder integrates knowledge-based decision-making by including investigation, prediction, and simulation iterative cycles for application in problem contexts where situation awareness is fluid or incomplete between decision agents. Solutions subsequently emerge from a gestalt of agent situation awareness, prior learning, and simulated action-execution-feedback cycles.

Al is currently developing rapidly, with new foundation models being adapted for specific and general purposes appearing almost weekly (Wang et al. 2023). System agents developed using machine learning approaches can be deployed to transform complex situations into actionable information that can help reduce decision-making from knowledge-based problem-solving to a rule-based/associative mapping decision in part or in full. As the technology develops further, it will have the potential to help deal with uncertainty, perform large-scale simulations of outcomes, and replace deterministic rule sets with self-learning pattern recognition to increase adaptability to new situations. However, as previously discussed, caution must be taken to preserve cognitive heterogeneity between decision-making agents through task, agent, and interface design. Such design should consider that the human counterpart in an Al-powered automated decision environment possesses by virtue of expertise a superlative ability to utilize intuitive cognitive processes to perceive, comprehend, and project (within a shared SA framework) either at the global maxima (goal) or local minima (action) of a decision to intervene in the decision process to force an adaptation of an action or goal.

Pertaining to the decline in expertise, contemporary findings in neuroscience reveal that expertise is not necessarily lost, but rather, the velocity of decision-related action potentials is reduced (Yewbrey et al. 2023). Indeed, the evidence suggests that a mechanism exists to "package" expertise (tacit knowledge and sequencing) into neural substructures. That is, moving structured motor and cognitive, logical sequences from the low-level specification of individual actions to high-level sequence features (or heuristics), thus moving from system two explicit planning and action to system one intuitive action.

#### 4.3 Designing Autonomous Execution

To realize autonomous end-to-end business processes, one or multiple Al-based automation agents must possess the capacity to contribute to the situation awareness and decision elements of the SADE Ladder (see Fig. 2) in order to respond to a new situation and make situated decisions without human intervention. In addition, these agents must have the functional agency to implement the decision in the form of concrete actions to complete the end-to-end decision loop. The execution of a decision can develop into more or less complex processes depending on the need, which can include the creation of an execution plan, the selection of the resources needed to enact the plan, and the performance of individual actions.

This aspect of execution is the right pillar of the SADE Ladder. Like situation awareness and decision-making, the execution pillar relates to a very different body of research and touches on the question, "what is the difference between automation and autonomy?"

Traditionally, workflows have been used to standardize and automate business processes (Riss et al. 2005; Schulz and Orlowska 2004). An AI-based workflow would be a workflow that is either triggered by an Al-based awareness agent or launches Al-based automation agents to perform steps within the larger workflow. However, this concept falls short, given that a network of cooperating multiple agents differs from a simple workflow model in that each participating agent generates an action plan based on its situation awareness and decision-making capabilities.

An example of complex action planning is Al-based logistics systems (Chien et al. 2020; Afanasenko and Borisova 2021; Winkelhaus and Grosse 2020), which plan and execute a series of business functions in response to a customer order, such as allocating inventory, scheduling picks in the warehouse, and booking carriers to get the items delivered from the fulfillment center to the customer. In this example, multiple systems must make many micro-decisions about the best fulfillment strategy and which resources to select to enact the fulfillment. Multiple independent agents will typically cooperate in executing such a complex fulfillment plan. A sourcing agent might find the nearest inventory, a dynamic tasking agent may assign the picking tasks to human shop floor workers or robots, and a transport scheduling agent may dispatch trucks to deliver the items as part of an optimized multi-stop route. Each step is optimized and enacted by domain-specific automation agents.

Such action-execution is typically the domain of workflow engines (Stohr and Zhao 2001), which perform a series of predefined business operations or system operations to achieve a particular outcome according to some predefined set of rules. In the above example, within the SADE Ladder, a network of cooperative automation agents transforms this deterministic workflow paradigm into an adaptive workflow, wherein agents make intelligent micro-decisions to optimize each step within the larger fulfillment flow to respond to needs stochastically. Each agent acts on domain-specific situation awareness (SA) and makes decisions (D) to define the best action plan (E).

In many cases, decisions depend on the simulation of actions. If, as a driver, one sets the destination of a subsequent trip, the navigation system of a self-driving car will recommend different routes based on the estimated driving time, traffic forecast, and other factors of relevance per its configuration. Based on the system-generated recommendations, the driver will decide which route to take and then delegate the navigation back to the navigation system. In this scenario, the navigation system is primarily the execution system but facilitates decision-making regarding choosing the optimal route by generating and simulating decision options, presenting this information to the human operator, who ultimately decides to either accept the recommended route or choose one of the alternative routes available (or rejecting all system-recommended routes, thus taking full control of the car and driving it).

Another example of an execution agent is the parking assistant: the driver decides where to park and then explicitly initiates the parking assistant to execute the parking maneuver. Again, the execution agent may facilitate the decision-making by notifying the driver if the parking slot is big enough to park, but its primary purpose is to execute the maneuver. The driver maintains the ability to cancel the parking maneuver if desired.

Therefore, to implement fully autonomous business processes, multiple agents must be orchestrated to cover the situation awareness, the decision-making, and the execution of the decision. Imagine a scenario where the parking assistant would be a fully autonomous agent that uses the destination of a car navigation system as the input. Based on this input, the parking assistant would decide what type of parking to use and automatically route the car to the parking garage or a reserved parking slot on the street, stop the car, and maneuver into the parking slot.

Following best practices of workflow modeling and human action planning, a human or a system agent must be capable of covering the following three aspects:

- **Orchestration:** Defining what type of resources are needed and in what order, steps, or services should be executed. This step is typically associated with the selection of the basic overarching strategy. For example, if one decides to take public transportation to the airport, one will plan the trip by considering public transportation infrastructure.
- **Resource allocation:** Once the overall plan has been defined, all resources must be secured and allocated in advance of executing the orchestrated plan. For example, if one decides to take a taxi to the airport, one must select an existing taxi service that, according to available data and experience, has a reasonable price, quality, and availability.
- **Action:** This is the actual enactment of a plan. It could, for example, be placing the order for a taxi service.

As with the other two phases of the conceptual framework, the levels of action planning are optional and depend on the situation's complexity. If procedures are well-known, there is no need for ad hoc action orchestration. If a simple action is needed (e.g., changing the price of a product), then no resources are needed either. The more strategic a decision is, the more likely the upper levels of each phase are involved.

Executing a decision is often not considered "intelligent," given the assumption that the action plan is fully specified. However, this is only true if the decision maps a situation to a known procedure, and the steps of this procedure are fully specified so that it can be executed canonically without further situational adjustments. However, the reality is that even simple workflows often include micro-decisions to follow policies and business rules. It is for this reason that Business Process Modeling and Notation (BPMN) representations are often complemented by Decision Model and Notation (DMN) representations that, when combined, model complex rule sets to determine the exact steps to take depending on available context variables (Aguilar-Savén 2004).

If we consider other domains like cockpit automation (Wiener 1988) or driver assistance systems (Piao and McDonald 2008), there are many cases where the purpose of the Al system is primarily execution and not decision-making. For example, in a fly-by-wire mode, the pilot makes decisions about maneuvering the airplane, but the system electronically executes the pilot commands by performing a sequence of flight parameters and mechanical parts adjustments. In the case of a car, the driver might decide to park and delegate the execution to the parking assistant system, which then parks the car on behalf of the driver. In both examples, the user is making the decision or expressing the intent, and the Al system's task is to execute the decision.

Execution is offloaded to systems for enhanced productivity and management of complex dependencies and constraints during plan generation. These systems decipher extensive dependency trees and optimize resource utilization, such as logistics and shop floor task scheduling. Additionally, their performance often surpasses human abilities, exemplified in driver assistance and robotic surgery. These systems necessitate dynamic adjustments, environmental comprehension, and precise motion, transcending basic automation.

One could argue that during execution, numerous micro-decisions must be made, with their nature contingent upon perspective. Decision-making may involve generating action plans as simulated scenarios for comparison against a baseline (current reality). This type of situation does not negate the SADE Ladder; rather, it implies that fully autonomous business processes necessitate orchestrating multiple automation agents, each with its loops within the SADE Ladder for autonomous action generation.

#### 4.4 Designing for Artificial Intelligence Transparency and Explainability

The relevance of transparency and explainability in the context of designing Al systems is well-known (Ehsan et al. 2021). With Al systems interpreting information, recommending decision options, making decisions, and planning actions, there is an increased demand for transparency of the cognitive capabilities of the underlying Al system and for the explainability of specific insights, recommendations, and actions.

The SADE decision ladder, with its phases and cognitive sub-functions, can serve as a conceptual model that illustrates how a multi-agent network contributes to an understand/decide/act task cycle or a more complex analyze/simulate/plan/act journey. Intelligence is distributed not only between human experts and Al agents but also between Al agents across those phases and different sub-functions. This underpins the importance of helping users understand the behavior of AI systems and the locality of artificial intelligence contributing to the outcome. We could argue that this concept model describes an Al-based tool's "under-the-hood" system reality. However, since the interaction dynamics between human experts and AI agents resemble a teaming approach, it is of utmost importance to help users establish a proper understanding of the system's capabilities, boundaries, and limitations to gain trust and maximize its utility.

One aspect of transparency is understanding the contributing network of agents. In her reappraisal of the situation awareness (SA) model (Endsley 2023), Endsley expands on the notion of awareness by merging the initial conceptualization of SA with human-Al collaborations to arrive at three levels of collaborative SA: taskwork SA, agent SA, and teamwork SA. Furthermore, Endsley states that human-Al SA is best supported by Al transparency that is both current and prospective.

Similar to human collaboration, one member must understand other members' capabilities and motivation and how work is orchestrated among the members. The SADE framework provides some guidance on how to communicate such a concept model by grouping cognitive functions into phases and incremental levels of cognitive complexity. This creates a lineage of Al-based capabilities and outcomes, which is important to understand when interacting as a user with an Al system that is composed of multiple Al agents.

For example, suppose a workforce planner disagrees with the number of hourly workers scheduled by the system for a given time of the day. In that case, the source of the disagreement may be the planner's perception that there is an inaccurate demand prediction, a situation awareness function; an erroneous conversion of labor demand to resources, a decision strategy function; or the wrong assignment of resources, a decision implementation function. Each function may be provided by independent complex Al systems, including personal assistants for each hourly worker who act as a broker between the shift planning engine and the preferences of one worker. In the absence of an underlying mental model, the workforce planner simply perceives inappropriate staffing and will not accept the system-generated plan. If she knew about the locality of the problem (e.g., demand, labor index, profiles of hourly users) or would be aware of the applied optimization strategy (e.g., costs vs. customer experience vs. hourly worker satisfaction), she would not only have more trust but also may be able to intervene at the right step of the chain of incremental decision-making and provide qualified feedback. This example illustrates how a black-box approach, which does not provide any insight into the locality and orchestration of Al systems, may lead to the rejection of the entire Al solution since the human expert cannot rationalize system decisions and effectively intervene.

The SADE Ladder also depicts incremental or alternative cognitive functions inside one phase, with higher-level functions increasing the desire to understand why and how an Al system came to a specific conclusion, decision, or action plan. It makes a difference if a decision is based on a simple fact (the "signal" in Rasmussen's model) or is based on a cognitive comprehension, projection, and classification process that leads to an insight that triggers a rule-based decision. In this case, the human expert may want the system to explain why an Al agent came to a specific conclusion or decision. The SADE Ladder suggests that explainability must be aligned with the available cognitive resources and support backward navigation along the chain of thought within the ladder.

Both transparency and explainability are important design principles to empower the human expert to understand and contribute to a decision-making process and to maintain a level of situation awareness (passive in the loop) so that intuitive processes such as pattern recognition are engaged to allow a sudden takeover of control without reconstructing situation awareness from scratch (Cummings and Guerlain 2005).

### 5 Summary

While Rasmussen's decision ladder and Endsley's situation awareness model are not perfectly aligned, by harnessing the respective strengths of each perspective, the integrative view afforded by the SADE Ladder can help guide the design of successful human-system-agent interactions that preserve cognitive heterogeneity across both human and Al agents.

Regarding cognitive functions and skills, both models introduce different cognitive sub-functions or mental processes for a single phase in the automation loop. Wherein, the cognitive resources (mental workload) required in both models increase similarly in each phase, i.e., levels 1-3 of situation awareness and skill-based, rule-based, and knowledge-based in decision-making. Besides combining those two pillars, the SADE Ladder extends this concept of increasing cognitive demands to the execution phase by performing actions, specifying actions, and orchestrating action plans.

Using a conceptual framework such as the SADE Ladder allows us to better understand the ergonomic aspects of embedding Al systems into the workplace and the interaction dynamics between human and Al teaming to get work done.

The SADE Ladder allows us to discuss human-centered aspects of Al-based task and process automation holistically across historically disconnected research domains. Combining research on situation awareness, decision-making, Computer-Supported Collaborative Work (CSCW), and business workflow gives us a new wide-angle lens to look at the challenges and opportunities of Al systems in the workplace. The scope of the design of Al systems goes far beyond the user interface; it expands to human-Al teaming and how to achieve transparency and controllability of hybrid socio-technical systems where multiple human experts and Al agents deliver meaningful business outcomes.

The anatomy of the SADE Ladder also suggests that an end-to-end decision loop covering a meaningful task or responsibility will depend on the orchestration of multiple cognitive sub-functions distributed within and across the three pillars assigned to one or multiple human- or Al-based agents. Such Al agents can be specialized in one cognitive function (e.g., detection of a moving object), on a pillar (an Al-based surveillance system monitoring a house and notifying the homeowner in case an unknown person is entering the property), or across pillars (a surveillance system interrogating the intruder and calling the police if not identified as entitled to enter the premise).

The SADE Ladder also makes it evident that different decision modes suggest structurally different networks of Al-based automation agents. At the same time, the situational adaptability of the decision loop to different levels of decision complexity is essential. We posit that taking an integrative approach to human-Al teaming that balances these aspects and trains human experts to utilize the strengths of intuitive decision-making while simultaneously providing an Al agent to automate and enhance system two-style decision-making would benefit a human-Al collaborative context. This strategy maintains cognitive diversity in the human-Al partnership, potentially mitigating concerns like overreliance, habituation, and distrust. Nevertheless, this method necessitates a decision-making framework that enables humar experts and Al to effectively utilize critical decision processes for task completion, for which we posit the SADE Ladder.

## The Dynamics of Human-AI Interactions in Organizational Decision-Making

**Abstract**  
Information systems (IS) are increasingly being used to support more complex and knowledge-intensive decision-making tasks. Contemporary systems often rely on machine learning, artificial intelligence (AI), and deep learning methods. While these technologies are promising in processing complex problems and large datasets, their black-box nature poses challenges for these systems' adoption in organizational settings, where understanding the rationale used by Al-driven decision-support tools is important. This study examines supply chain experts' interactions with Al systems through the lens of the situation awareness (SA) model. We identify factors leading to a user's decision to either delegate decision-making tasks to Al or intervene in the Al-driven processes. Our findings suggest that mismatches between the Al's SA and the expert user's SA create tensions that impact the delegation of tasks to the Al system. We discuss additional sources of tension and propose human-centered design practices to mitigate these issues.

### 1 Introduction

Information systems (IS) play a critical role in enabling employees to make better decisions in today's organizational environments. With continuous technological improvements in data processing capabilities, decision support has evolved from simple computations to providing recommendations in complex problem-solving scenarios. Consequently, this shift has transformed IS from simple tools to artificially intelligent agents that support, and in some cases, work together with, domain experts. This shift highlights the importance of collaborative decision-making models between humans and Artificial Intelligence (AI), which are becoming more common in organizational processes (National Academies of Sciences, Engineering, and Medicine 2021).

As the extent of decision support from IS extends beyond simple calculations to autonomous decision-making, the relationship between the user and IS becomes more complex. Users may either rely excessively on the IS, potentially losing their ability to understand how future events impact their decision-making environment, or distrust the IS, thereby not fully benefiting from its capabilities in processing large volumes of data and deriving relevant insights (Chiang and Yin 2022). Balancing these two tendencies is critical for optimal operational outcomes.

Previous research has extensively studied the impact of IS investments on organizational outcomes (Brynjolfsson 1993; Sabherwal and Jeyaraj 2015). Extant theories and models, particularly the technology acceptance model (TAM) and its extensions, highlight factors such as perceived usefulness and ease of use as drivers of IS adoption (Davis 1989; Venkatesh and Bala 2008). While TAM has been used extensively to explain user interactions with traditional IS, the complexities of Al-powered systems necessitate further research on the topic. Unlike traditional IS that function within defined parameters, Al systems are dynamic and adaptive. In other words, Al systems possess the capability to continuously learn and modify their operational patterns based on new data inputs (Schuetz and Venkatesh 2020).

While the benefits of using Al in complex decision-making tasks are clear, this continuously evolving and often opaque nature of Al presents unique challenges (Endsley 2022). One such challenge is understanding how users perceive and adapt to a dynamic Al-driven decision support system and how they make ongoing decisions regarding task delegation to Al. Therefore, in this chapter, we aim to answer the following research question:

> RQ: What factors influence professionals' decisions on whether to rely on Al in organizational decision-making processes?

To understand users' decisions about delegating tasks to Al, situation awareness theory emerges as a promising lens (Endsley 1995). Situation awareness (SA) is defined as "the perception of the elements in the environment within a volume of time and space, the comprehension of their meaning, and the projection of their status in the near future" (Endsley 1995, p. 1). While the SA concept was initially defined to understand and explain human decision-making with automation tools in aviation, it can be extended to the context of human-Al interactions (Endsley 2022).

This chapter presents a study conducted to answer our research question by considering the perspectives of professionals shared through expert interviews. The rest of the chapter is organized as follows: first, we provide an overview of the SA concept and its relevance to human-Al interactions. We then describe our methods for collecting and analyzing stakeholder interview data within a supply chain management (SCM) context. Results are discussed in two parts: first, we introduce six contextual factors influencing users' meta-decisions whether to delegate decision-making to Al; then, we incorporate these factors into the SA model and propose a conceptual extension. The chapter concludes with key considerations for successful Al implementation projects.

### 2 Background

#### 2.1 The Adoption and Use of IS and Al

Information systems have been integral to organizational decision-making environments for several decades. As the success of an IS depends on its use, the factors leading to acceptance have been extensively studied in the literature. The technology acceptance model (TAM) (Davis 1989) is one of the well-known perspectives, which presents perceived usefulness and perceived ease of use as predictors of user acceptance. Over the years, the set of constructs predicting user acceptance has expanded, leading to models such as TAM2, the unified theory of acceptance and use of technology, and TAM3 (Venkatesh and Davis 2000; Venkatesh et al. 2003; Venkatesh and Bala 2008).

While these attitude-based theories and other richer theoretical lenses provide valuable insights regarding the use of traditional IS, they assume that an IS is deterministic and behaves predictably according to the rules it was designed to follow (Schuetz and Venkatesh 2020). Al, however, does not rely on a predetermined set of rules. Instead, Al-driven systems continuously learn from new observations and adapt their behavior accordingly (Czako et al. 2021). Therefore, to address the need to better understand human-Al interaction dynamics in organizational decision-making, we borrow the concept of situation awareness from the scientific discipline of human factors.

#### 2.2 The Situation Awareness Model

Situation awareness (SA) is defined as "the perception of the elements in the environment within a volume of time and space, the comprehension of their meaning, and the projection of their status in the near future" (Endsley 1995, p. 1). This definition of SA is human-centric, focusing only on the human operators. According to this model, situation awareness is characterized by three levels:

1. **Perception of elements in the current situation:** This consists of the observations of data points relevant to decision-making.
2. **Comprehension of the current situation:** This level is about the use of the SA elements from the first level to create a meaningful synthesis that represents the current reality.

Pages 32 to 33 are not shown in this preview.  
collection, the three grounding phases were followed iteratively within and across participants.

### 4 Results

As this study's research focus was on understanding the dynamics between human and Al systems during decision-making, we structure the findings across decision-making process stages. A typical decision-making process involves three key stages: sensing, deciding, and acting (Endsley 1995). As we consider human decision-making processes assisted by Al, we focus on the first two stages, sensing and deciding, where the sensing step also consists of three levels, akin to the three levels of SA (Endsley 1995): detect, comprehend, and predict.

#### 4.1 Decision-Making with Al Support and Meta-decision-making

Our findings confirm that decision-makers often follow the three-step decision-making process when faced with decision-making problems, but with a slight difference caused by the presence of Al support. SCM decision-makers tend to view Al more than mere tools. Instead, they consider Al as their counterparts working in the same decision-making process. They continuously evaluate the Al's performance and reliability at various stages of the decision-making process. We refer to this ongoing evaluation and reflection of the decision-making process and strategy as meta-decision-making.

Meta-decision-making can be defined as "deciding how to decide" (Kottemann 1986). It involves choosing the techniques and information that will guide the primary decision-making process. In the context of human-Al interactions, our results indicate that the decision to delegate certain aspects of the decision-making process to Al is not a one-time choice but a continuous consideration.

Our results suggest that depending on which stage of the decision-making process is evaluated, different factors influence the meta-decision of delegating a decision to an Al-system or not.

The sensing phase of the decision-making aligns with the three levels of situation awareness: detecting, comprehending, and predicting. Given the central focus on situation awareness, we have organized the results section according to the stages of decision-making process and the different levels of SA as described by Endsley. We discuss these influencing factors summarized in Table 1 in the following sections.

#### 4.2 Factors Influencing the Meta-decisions at the "Sense, Detect" Phase

Our results indicate that decision-makers are comfortable with delegating the perception of elements in the current situation when they are confident that it is safe to do so. In the context of Al-supported decision-making in supply chain management, this confidence depends on factors such as input data, data volume, complexity, and workload

##### 4.2.1 Input Data

An Al system's SA depends on the quality and relevance of the data it receives as input. Our interviews revealed that discrepancies often exist between the data Al systems use and the data human experts consider relevant, especially in exceptional circumstances—for example, when plans deviate from the baseline values. Participants noted that the Al system lacked awareness of the root cause of such deviations due to missing variables or data points in their dataset. If the Al system does not allow users to add these additional facts as input, users may hesitate to delegate the subtask of monitoring the environment to the Al. Since detecting the environment is the first step in the decision-making process, choosing not to delegate this task means that users will likely complete the entire decision-making process manually.

> (Decision-makers) want to see every price because they are used to doing that. They know they spend a lot of time, but they still want to continue. They say—I want to look at each and every price, and if I am okay (with that), I will go to the next step. - Participant 4

Participants frequently reported evaluating the Al's perception of reality, especially when they are trying to learn more about a newly installed system's abilities. Establishing and communicating that the Al correctly perceives reality is essential before users feel comfortable delegating subsequent stages of SA and decision-making to Al. Without this assurance, users may prefer not to delegate any steps of the decision-making process.

##### 4.2.2 User Control on Input Data

In addition to the input data the system considers, users' ability—or lack thereof—to configure or adjust input variables influences their meta-decision to delegate the first level of SA to an Al. For instance, one participant explained that they had no way of knowing whether the Al planning tool accounts for an employee's experience level, which can impact the expected time required to complete tasks. Without a way of understanding or adding such contextual variables, users often take over the task at the sensing stage instead of delegating any decision-making responsibilities to AI.

> I just heard ten new associates (just started), and they are not experienced. So, I know that I can not look at the freight planning tool and put that exact amount of people (based on what the planning tool says). I am going to need a little bit more (associates) because they are going to be slower. — Participant 8

When exceptions in the environment that the Al cannot perceive occur frequently, users may begin to disregard Al outputs in subsequent stages of the decision-making process. In other words, they dismiss the Al's model of reality and its predictions for the future, based on their perceptions that Al lacks an accurate perception of reality to begin with.

> There are some circumstances that we make a business decision (to apply) aggressive promotions due to market conditions, and we switch this (Al forecasting) system off for this period. - Participant 3

These findings highlight that an Al's ability to incorporate user input related to decision-making environment is critical at the first stage of decision-making, sensing the environment. Moreover, the results indicate that a user's choice not to delegate the perception of the environment to Al can lead to the dismissal of Al throughout all subsequent stages of decision-making.

##### 4.2.3 Data Volume

Another factor that influences the meta-decision of whether or not to delegate a task to Al is the volume of data that must be processed to develop the SA required for a business decision. Our interview data indicates that users rely on Al not only to improve decision accuracy but also to handle vast quantities of data points that would be challenging or impossible to process manually. Participants stated an increased possibility of delegating the task to Al as the data volume increases.

> Having millions of items, ... it becomes very difficult to do it manually. That is where it (Al-driven decision support system) comes in. It automates the manual task, and it is highly scalable. - Participant 4

Participants reported that when working with large datasets, they often choose to delegate the analysis of individual data points to Al, effectively delegating the initial phase of SA development to the system. According to our interview data, this delegation allows them to focus on higher-priority tasks, as they are freed from the time-intensive task of developing the first level of SA from large datasets.

#### 4.3 Factors Influencing the Meta-decisions at the "Sense, Comprehend" Phase

##### 4.3.1 Data Volume

As reliance on data increases in SCM, users report having limited time to evaluate every detail, often finding it necessary to delegate subsequent stages of decision-making to Al.

> There are thousands of (Al-generated decision) proposals each week and (we have) only six parallel meetings of one hour each (to evaluate these proposals). - Participant 3

Our interview data suggests that the greater the volume of data, the more likely users are to rely on Al not only to build the initial level of SA but also to handle later decision-making stages.

##### 4.3.2 System Transparency

System transparency refers to "the understandability and predictability of the system" (Endsley et al. 2003, p. 146). A highly transparent system allows users to understand the data sources and inner logic behind its predictions (Chen et al. 2014). Our findings indicate that users' expectation regarding the level of transparency may change according to task type, complexity, and user preferences.

> (some users) want to challenge everything and they are never happy with the solution, and they look at it as a suggestion and then they're constantly trying to modify it. — Participant 12

Participants also stated that their trust in Al is influenced by system transparency, which in turn affects the degree of control they allow Al to exert in the decision-making process.

> All it said was these (are) your demand hours based on your trends, and these are the hours. ....I like to understand how things work. - Participant 8

Since system transparency is essential for understanding how an Al system constructs a model of the environment, it influences users' decision to delegate the development of level 2 SA to AI.

Pages 39 to 40 are not shown in this preview.

### 5.2 System Design as a Factor in Meta-decisions

When users perceive a misalignment, a tension manifests (Jiang et al. 2023). In such cases, experts who are highly confident in their domain knowledge may rely more heavily on their own SA than on the Al's. This perceived misalignment is often amplified if users believe that Al lacks a complete dataset and, consequently, an adequate SA. In such cases, giving users the tools to manage and address this lack of perceived misalignment becomes critical to Al's long-term adoption in decision-making. Our findings indicate that three system design factors impact users' meta-decisions: Al transparency, Al explainability, and Al configurability.

Participants noted that before committing to a decision and taking action in complex organizational contexts, they often ensure they can later justify their decisions. This need for justification of decisions makes it more important for the Al recommendations to be understood in terms of the logic behind the decision. Therefore, system transparency and the level of explanation detail within the user interface are crucial factors in determining users' meta-decisions to delegate to Al.

Participants also frequently cited situations in which they held contextual knowledge-such as upcoming competitor discounts or unique seasonal events—that the Al was unaware of. While participants stated their preference toward Al systems that allow them to adjust for such additional variables, they also indicated that when Al does not allow such adjustments or configurations, they tend to retain control over the decision-making task rather than delegating it to Al.

### 6 Discussion and Conclusions

This study explores the dynamics of human-Al decision-making in complex environments, particularly in the context of supply chain management (SCM). After interviewing 20 SCM professionals experienced in using Al to support their decisions, we developed an extended decision-making process model that highlights how they make continuous meta-decisions on task delegation to Al. This model emphasizes Al as an autonomous agent with its own situation awareness (SA), which is shaped by its data inputs and evolving learning capabilities. By examining user-Al interactions through this SA alignment framework, our research provides insights into how alignment—or lack thereof-between user and AI SA influences task delegation to Al.

Our findings reveal that human-Al decision-making is a dynamic process shaped by the interplay of user SA, AI SA, and a continuous cycle of meta-decisions. Users are more likely to delegate tasks to Al when they perceive a high alignment between their SA and the Al's SA. This alignment sets the foundation for trust and influences the extent to which users feel comfortable in delegating tasks to Al

Finally, our study identifies transparency, explainability, and configurability as key system design factors that influence users' meta-decisions. Users stated relying

You have either reached a page that is unavailable for viewing or reached your viewing limit for this book.

## Part II

### The Design Challenge of Using Intelligent Agents to Make Work Systems Smarter

Steven Alter

#### Abstract

This paper presents a new approach to the design challenge of using intelligent agents to make work systems smarter. It starts by summarizing the main ideas in a work system perspective that outlines the context. After adding concepts and assumptions concerning intelligent machines, it discusses the smartness of systems or devices in terms of capabilities grouped under the categories of processing information, self-regulation, action in the world, and knowledge acquisition. It presents a spectrum of roles that intelligent agents might perform for a work system. It also presents facets of work (such as making decisions, communicating, and coordinating) to which any of those roles might be applied. It presents an example to illustrate how those ideas might be used and concludes by summarizing its integrated approach to designing intelligent agents to make work systems smarter.

#### 1 Introduction

The design challenge of using intelligent agents to make work systems smarter requires integrating ideas about work systems and ideas about intelligent agents despite inconsistent terminology in each area. For example, the words intelligent and smart have been applied to many things that are largely unrelated, e.g., smart watches, smartphones, smart cities, smart contracts, and smart refrigerators. Explaining how intelligent agents might make work systems smarter requires defining those terms and explaining how they can be used in an integrated way. This paper aims to provide an organized way to think about opportunities for obtaining better results by making systems more intelligent even when different observers disagree about their intelligence. It applies ideas developed at different times to contribute to the real-world design of socio-technical systems that use entities that might be described as smart or intelligent in some way. Those ideas assume that metaphors and yes/no definitions of smart or intelligent are not directly useful for design or analysis, even though they may be useful for analyzing survey research results, for punditry, or for philosophical discussions about visions of the future. Instead, ideas presented here highlight a spectrum of possibilities that design efforts might try to achieve to varying extents depending on available resources and needs.

#### Organization

This paper combines older ideas about work systems with ideas from recent articles related to the smartness of systems and devices, facets of work, Al usage as the usage of digital agents, and an agent-role framework called the agent-responsibility framework in earlier publications. Section 2 summarizes the main ideas in a work system perspective based on work system theory. Those ideas are fundamental for understanding the context for designing intelligent agents to make work systems smarter. Section 3 adds concepts and assumptions concerning intelligent machines that are inherent in this paper's interpretation of using intelligent agents. Section 4 unpacks the idea of smartness of devices and systems based on degrees of smartness and groups of capabilities that might be smart to varying degrees. Section 5 combines the ideas from previous sections by identifying a spectrum of roles that intelligent agents might perform for a work system and facets of work (such as making decisions, communicating, and coordinating) to which any of those roles might be applied. Section 6 summarizes this paper's integrated approach to designing intelligent agents to make work systems smarter.

#### 2 The Work System Perspective

The work system perspective has evolved over three decades. It covers all operational systems in organizations, including totally automated WSs and socio-technical WSs that use intelligent agents. It covers systems built to aid humans in specific human tasks, systems that use automation to replace people who previously performed specific tasks, and systems that perform totally automated tasks that were never performed by people. Thus, it covers both work systems with various forms of interactions between humans and computers and work systems where responsibilities of humans and computers are structured to be completely separate.

The development of the work system perspective started with an attempt to create a systems analysis method for business professionals, which was articulated as the work system method (WSM)-(Alter 2006, 2008). The ideas underlying WSM were formalized as work system theory (WST). Subsequent developments related to service systems, workarounds, design principles, and other topics are extensions of WST (Alter 2013). WST applies equally to WSs in general and to ISs, projects, totally automated work systems, and other special cases of WS. As will be discussed briefly, the WST's three components are the definition of WS, the work system framework, and the work system life cycle model. Since these ideas have been presented many times, this section will focus on key points to minimize misunderstandings of the entire approach.

##### Work

For current purposes, work is defined as the use of resources to produce products or services for human or non-human customers or for oneself. Resources include human, informational, physical, financial, and other types of resources. Work involves activities that try to be productive and may occur in any business, societal, or home setting. This definition of work is not directly related to careers, jobs, or business organizations.

##### Work System

This term appeared in the first edition of MIS Quarterly (Bostrom and Heinen 1977) and is a natural unit of analysis for thinking about systems in organizations. A work system is a system in which human participants and machines perform work (processes and activities) using information, technology, and other resources to produce specific products or services for internal or external customers. Nontrivial work systems typically involve multiple participants and machines serving multiple customers. The work in work systems may be structured to varying degrees, e.g., unstructured (designing a unique advertisement), semi-structured (performing typical medical diagnosis), workflows (processing invoice payments), or highly structured (manufacturing semiconductors or pharmaceuticals).

##### Workplace

A physical or virtual place where work is performed. Except for situations where most work is highly improvised, most workplaces contain multiple work systems that perform different aspects of the required work. Also, most nontrivial work systems can be subdivided into interacting subsystems that are work systems in their own right.

##### Special Cases of Work System

The most important distinction in describing special cases of WS is the difference between a socio-technical WS, in which human participants perform some of the activities, and a totally automated WS, where all activities are performed by machines and the slot for participants is blank.

An information system (IS) is a WS with activities devoted to capturing, transmitting, storing, retrieving, deleting, manipulating, and/or displaying information. This definition differs from 20 previous definitions in Alter (2008) and was one of 34 definitions of IS noted in Boell and Cecez-Kemanovic (2015). An example is a socio-technical accounting IS in which accountants decide how specific transactions and assets will be handled for tax purposes and then produce monthly or yearly financial statements. This example is an IS because its activities are devoted to processing information. It is supported by a totally automated IS that performs calculations and generates reports. In both cases, an IS that is an integral part of another WS cannot be analyzed, designed, or improved thoughtfully without considering how changes in the IS affect that WS.

Projects, service systems, self-service systems, and some supply chains (interorganizational WSs) are other important special cases. For example, software development projects and others are WSs designed to produce specific products or services and then go out of existence. Thus, a project that creates or customizes an intelligent agent is a WS in its own right.

##### Work system framework: a basic understanding of a WS

The nine elements of the WS framework (Fig. 1) are the elements of a basic understanding of a WS's form, function, and environment during a period when it is stable enough to retain its identity even though incremental changes may occur, such as minor personnel substitutions or technology upgrades. Processes and activities, participants, information, and technologies are completely within the WS. Customers and products/services may be partially inside and partially outside because customers often participate in activities within a WS and because products/services take shape within a WS. Environment, infrastructure, and strategies are outside of the WS even though they have direct effects within a WS and may be affected by major changes in significant WSs.

The following clarifications are often useful: Customers refer to people or organizations that receive and use products/services produced by a WS. It includes internal and external customers. The term product/services is used to bypass controversies about special characteristics of products versus services. The term processes and activities is used because the activities in some WSs are not structured as explicitly defined processes. Environment includes organizational and national culture, competition, regulation, demographics, and organizational politics, policies, and procedures. Infrastructure refers to human, informational, and technical resources that are viewed as shared by multiple WSs instead of being associated primarily with one WS. An example of human infrastructure is an IT group that serves as a resource for multiple WSs.

##### Work System Life Cycle Model (WSLC): How WSs Change over Time

ISs and other WSs evolve through a combination of planned change through projects and unplanned change through adaptations and workarounds (Fig.2). WSLC phases (initiation, development, implementation, operation, and maintenance) may be performed differently. Typical activities and responsibilities, e.g., designing, debugging, and training, associated with specific phases apply for waterfall, agile, prototyping, use of off-the-shelf applications, and shadow IT, even when several phases overlap or iterate.

Both planned and unplanned changes often affect multiple WS elements, not just technologies. The development phase creates or acquires and then tests software and immediate action based on environmental stimuli) or proactive (taking initiative and planning actions in advance).

##### Digital Agent

Digital agents operate by executing algorithms encoded in software. A digital agent can be viewed as a work system because it performs work using information, technologies, and other resources to produce products/services for its direct customers, which may be human or non-human entities.

##### Intelligent Agent

An intelligent agent operates by executing algorithms and exhibits a nontrivial level of "smartness" with processing information, self-regulation, action in the world, and knowledge acquisition (categories explained in the next section). The operation of an intelligent agent can be viewed as a work system. This definition's use of the term nontrivial follows this paper's purpose of focusing on design choices that are significant enough to require deliberation.

##### Situation Awareness

This idea (SA) helps in seeing the value of this chapter's approach of considering different degrees of smartness related to different types of capabilities instead of seeing smartness in terms of metaphors or yes/no distinctions. A widely cited article about SA (Endsley 1995), starts with important examples related to aircraft crews, air traffic control, and complex manufacturing and power systems. It defines SA as "the perception of the elements in the environment within a volume of time and space, the comprehension of their meaning, and the projection of their status in the near future" (p. 36). By that definition, SA applies well to human operators in those situations. It applies in a more limited way to devices such as thermostats that operate through a simple algorithmic feedback loop between target and actual temperature with no complex rules, projections into the future, or knowledge-based decision-making. Recognizing the increasing power of automation, some researchers have proposed the idea of distributed situational awareness (DSA) to describe socio-technical systems in which "SA is held by human and non-human agents. Technological artifacts (as well as human operators) have some level of SA (at least in the sense that they are holders of contextually relevant information). This is particularly true as technologies are able to sense their environment and become more animate" (Stanton 2016, p. 2). This paper's discussion of using intelligent agents to make work systems smarter assumes that intelligent agents might exhibit SA in a variety of ways. It contributes to the discussion of SA and DSA by identifying many different types of capabilities that might perform roles in socio-technical or totally automated systems. Its focus on continuous variables related to different aspects of smartness provides ideas that are directly useful for analysis and design.

#### 4 Smartness of Intelligent Agents

A conceptualization of smartness of devices and systems is explained in Alter (2020a), which provides a general way to look at smartness using four categories to organize numerous capabilities that might be built into devices or systems and operate with different degrees of smartness.

The smartness of an intelligent agent or work system in relation to almost any type of capability that involves information can be characterized along the following dimensions (Alter 2020a):

- Not smart at all. Does not perform activities that exhibit the capability
- Scripted execution. Performs capability-related activities according to prespecified instructions
- Formulaic adaptation. Adaptation of capability-related activities based on prespecified inputs or conditions
- Creative adaptation. Adaptation of capability-related activities based on unscripted or partially scripted analysis of relevant information or conditions
- Unscripted or partially scripted invention. Invention of capability-related activities using unscripted or partially scripted execution of a workaround or new method

Most work systems with human participants have many capabilities under each of four groups of capabilities and may execute related activities with occasional adaptations or inventions. Each group of capabilities includes between five and seven separate capabilities that may exist independently or be intertwined with other capabilities:

- Information processing. Capture information, transmit information, store information, retrieve information, delete information, manipulate information, and display information.
- Action in the world. Sensing, actuation, coordination, communication, control, and physical action.
- Internal regulation. Self-detection, self-monitoring, self-diagnosis, self-correction, and self-organization.
- Knowledge acquisition. Sensing or discovering, classifying, compiling, inferring or extrapolating from examples, inferring or extrapolating from abstractions, testing, and evaluating.

With the possible exception of work systems that focus on totally routine and repetitive activities, most work systems with human participants operate with a nontrivial level of smartness for most of those capabilities. In contrast, very few intelligent agents exhibit a high level of smartness related to most of those capabilities. The design challenge of using intelligent agents to make work systems smarter involves designing intelligent agents whose limited smartness nonetheless enhances the smartness of a work system that uses intelligent agents.

#### 5 Making Work Systems Smarter

Incorporating an intelligent agent into an existing WS creates what might be called an augmented WS, i.e., the original WS augmented by the capabilities of the intelligent agent. The elements of the work system framework can be used to describe the structure and characteristics of both the original WS and the augmented WS. The topics for describing the augmentation by the intelligent agent are not obvious, however, because augmentation by an intelligent agent may change many aspects of the WS.

##### 5.1 Analyzing a Work System from a Business Viewpoint

The work system method (WSM) mentioned earlier provides an analysis approach for thinking about making work systems smarter. The WSM is a flexible systems analysis and design method developed to help business professionals analyze work systems for themselves, to support both their own work and their interactions with IT professionals in system-related projects. During 2003-2017 (mostly), MBA and EMBA students demonstrated the usability of that general approach by applying various work system analysis templates in assignments that produced over 700 management briefings about possible improvements in real-world work systems, mostly in their organizations, e.g., Truex et al. (2010). In every case, producing any required software would have called for additional analysis and specifications concerning the details of the software itself. Different versions of WSM have been tailored to different uses. However, the following WSM steps outline an analysis approach that can be taken much further when thinking about using intelligent agents to make work systems smarter:

1. Identify the smallest work system that has the problem or opportunity at hand.
2. Summarize the "as-is" work system using a work system snapshot, a stylized one-page summary that identifies the main constituents of the six central elements of that work system.
3. Evaluate the work system's operation using perceived strengths and weaknesses, metrics, key incidents, social relations, and other factors.
4. Drill down further as necessary using any relevant ideas, including both published WSM tools, e.g., Alter (2006), Alter (2020b), Alter and Bork (2020), and other tools and approaches such as design thinking and Six Sigma methods.
5. Propose changes by producing a work system snapshot of a proposed "to be" work system that should perform well.
6. Describe likely performance improvements and explain why the effort of creating the new work system or making the proposed changes seems justified.

##### 5.2 Roles that an Intelligent Agent Might Perform for a Work System

An attempt to use an intelligent agent to make a work system smarter might consider any of a spectrum of roles that an intelligent agent might perform in a WS. The spectrum of roles shown below goes from the lowest to the highest direct involvement of an intelligent agent in executing a WS's activities. That set of roles emerged from many iterations of trying to expand the horizontal dimension of Shneiderman's human-centered AI (HCAI) framework (Shneiderman 2020a,b), which applies to many situations where both automation and human control are design variables. The HCAI framework's two dimensions are low versus high computer automation and low versus high human control. Those dimensions directly lead to useful questions for evaluating and designing WSs by emphasizing that deficiency or excess along either dimension may lead to worse results for organizations, WS participants, and customers. For design, however, the low versus high distinctions in the HCAI framework's two dimensions provide too little detail to inspire vivid visualization and discussion of how or why an intelligent agent might be applied in a WS's operation or might affect its stakeholders.

One of the early iterations in trying to make the HCAI framework more specific involved three roles, i.e., support, control, and perform. Specific instances of those roles might support HCAI values and aspirations or might oppose those values and aspirations (e.g., micromanagement or surveillance capitalism). The following comments about the six types of roles emphasize promoting human-centric values and addressing human-centric issues:

- **Monitor a Work System**: An intelligent agent might monitor and measure aspects of work to ensure that a WS's processes and activities are appropriate for WS participants. In some cases, intelligent agents might generate alarms when aspects of work start going out of accepted boundaries regarding health, safety, or cognitive load.
- **Provide Information**: An intelligent agent might provide information that helps people achieve their work goals safely and comfortably without infringing on the privacy or other rights of people whose information is used.
- **Provide Capabilities**: An intelligent agent might provide analytical, visualization, and computational capabilities that help WS participants achieve their assigned goals safely and appropriately.
- **Control Activities**: An intelligent agent might control work activities directly to prevent specific activities from going out of bounds related to worker safety, time on the job, stress, or other variables that can be measured and used to control a work system.
- **Coproduce Activities**: An intelligent agent might be deployed in a division of responsibility where the intelligent agent and people have complementary responsibilities for performing separate parts of the work. Either humans or intelligent agents might take the lead in human-computer interactions within coproduction activities. In some situations, initiative might shift back and forth between people and intelligent agents depending on the status of the work.
- **Execute Activities**: An intelligent agent might execute activities that should not or cannot be delegated to people. For example, an intelligent agent might perform difficult, dangerous, or impossible activities for people to perform as the WS produces products/services.

##### 5.3 Facets of Work

Looking more deeply at how an intelligent agent might make a work system smarter calls for considering the possibility that any of the above roles might be applied to any of a set of facets of work, i.e., aspects of work that can be observed or analyzed, such as making decisions, communicating, processing information, and coordinating. The idea of facets of work grew out of research for bringing richer and more evocative concepts to systems analysis and design to facilitate interactions between analysts and stakeholders, as is explained in (Alter 2021, p. 342-344). The notion of "facet" is an analogy to how a cut diamond consists of a single thing with many facets that can be observed or analyzed. Psychology, library science, information science, and computer science have used the idea of facets but with quite different meanings and connotations.

All 18 of the facets of work in Table 1 apply to both socio-technical and totally automated WSs, are associated with many useful concepts, bring evaluation criteria and design trade-offs, have sub-facets, and bring open-ended questions for starting conversations (Alter 2021). Some facets overlap in many situations (e.g., making decisions and communication). The selection of the 18 facets resulted from an iterative design process that might have led to some other set of facets, perhaps 14 or 27. Determination of whether or not to include a type of activity as one of the 18 facets of work in Table 1 was based on the extent to which that type of activity was associated with concepts, evaluation criteria, design trade-offs, sub-facets, and open-ended questions that could be useful in analysis and design. The central contribution of facets of work for thinking about intelligent agents is that

| Making decisions       | Communicating       | Providing information   |
|-----------------------|--------------------|------------------------|
| Representing reality  | Learning           | Coordinating           |
| Performing physical work | Providing service | Applying knowledge     |
| Planning              | Improvising        | Performing support work|
| Creating value        | Thinking           | Controlling execution  |
| Processing information| Interacting socially| Maintaining security   |

#### 6 An Integrated Approach for Designing Intelligent Agents to Make Work Systems Smarter

This paper's main premise is that broad generalizations about intelligent machines and Al in the workplace are interesting in many ways but often are not specific enough to be useful for describing, analyzing, or designing WSs. The ideas presented here bypass ambiguities related to the terms intelligent agent or intelligent machine by providing an integrated approach to understanding intelligent agents and their roles in specific WSs in workplace settings. The basic assumption is that intelligent agents are used by WSs and that incorporating a new intelligent agent into a WS should augment the WS's capabilities in predictable ways that can be described based on roles that the intelligent agent plays concerning specific facets of work.

There are many possible paths for designing intelligent agents to make work systems smarter. The integrated approach proposed here starts by using the general outline of the work system method to understand the context and the related opportunities and problems. Additional ideas that can be used in the design effort include a spectrum of possible roles of intelligent agents, facets of work that can be affected by those roles, and different degrees of smartness that might be pursued concerning capabilities in four categories. This approach is not meant to be mechanical or formulaic. Instead, it is meant to illuminate possibilities for designing intelligent agents with maximum benefit.

The ideas presented here can be applied in describing, analyzing, and designing WSs in many workplaces by asking questions such as the following:

- How smart is the current WS concerning the various capabilities that were grouped previously under processing information, self-regulation, action in the world, and knowledge acquisition?
- What are practical directions for making the WS smarter by enhancing those capabilities in directions that might be beneficial?
- How do those possible directions link with the roles and facets of work identified using the AR framework?

An important caveat is that most intelligent agents have limited smartness in their own right due to the current limitations in how automated devices can understand their own purposes, behaviors, or environments. The key issue is to try to make the work system smarter through an integrated combination of changes, some of which might involve new or improved intelligent agents.

Many earlier papers have applied this paper's ideas about WSs in a variety of situations. However, the new ideas about the smartness of devices and systems and the roles of intelligent agents regarding facets of work within WSs have not been used in practice. Future applications in practice are needed to determine how useful they are for analyzing and designing intelligent agents and WSs that use them.

