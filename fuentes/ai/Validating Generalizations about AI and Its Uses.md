# Validating Generalizations about AI and Its Uses

## Abstract
Many discussions of the tremendously important
topic of AI are undermined by vague definitions of what
AI is and by generalizations that are too distant from
reality to be useful when pursuing or evaluating real
world applications. Inconsistent and exaggerated
definitions of AI published during 2019-2022 illustrate
the need for an approach to validating current
generalizations about AI and its uses and not just
repeating decades-old predictions and images based on
science fiction movies. Next, this paper presents a series
of evaluation issues for generalizations and a series of
ideas that are useful for describing AI applications as
uses of algorithms based on techniques associated with
AI. It incorporates those ideas into summaries of six
diverse applications of algorithms associated with AI. A
concluding section presents suggestions for realistic
descriptions of AI applications and for generalizations
about AI.

## 1. Impetus
This paper evolved out of research initially directed
at producing an essay about responsible AI (RAI) and
trustworthy AI (TAI). Initial steps toward literature
reviews soon revealed difficulty in verifying whether a
person or organization claiming to use AI in a specific
situation such as making hiring decisions or selecting
among university applicants actually is using AI. AI has
become a broad umbrella term encompassing many
different types of computerized capabilities that are
largely unrelated to each other, e.g., chatbots, search
engines, speech dictation, facial recognition, self-
driving vehicles, natural language processing,
generation of images, advice services, and so on. Some
statements presented as generalizations about AI are
actually about some of those types and are unrelated to
others. Divergent viewpoints of different stakeholders
in applying criteria such as efficiency, effectiveness,
equity, and explainability to specific situations make it
even more difficult to decide whether many instances of
AI usage represent either RAI or TAI. Somewhat
Steven Alter
University of San Francisco
alter@usfca.edu
similarly, Crowston (2022, p. 425) argued that “the term
‘AI’ does not help frame a research study as dozens of
technologies fall under this umbrella term and
applications can comprise a mix of them.”

Goal. This paper goes beyond merely saying that
many definitions and generalizations of AI fail to
provide accurate portrayals of many situations that are
called applications of AI. It also tries to go beyond the
almost cliched overviews of RAI and TAI by consulting
firms such as Accenture, Deloitte, and PWC and by
leading tech firms such as Google, IBM, and Microsoft.
Although the details available through their websites
vary somewhat, the related commentaries typically
highlight variations on ethics, accountability,
transparency, explainability, fairness, inclusiveness,
privacy, security, reliability, robustness, safety,
resilience, risk control, governance, and performance.
Most of those RAI and TAI ideas are relevant to almost
any important information system, regardless of
whether AI is involved in any way.
This paper’s goal is to provide a broadly applicable
approach for framing generalizations about AI that are
intended as more than casual observations, corporate
outreach, or predictions. Its ideas can be used in various
ways for describing and analyzing roles and impacts of
AI-related capabilities in work systems that use those
capabilities. Its ideas also can be used to evaluate
generalizations related to AI.

Organization. The next section illustrates the
challenge of validating generalizations about AI by
listing some of the many inconsistent definitions of AI
that have appeared in recent literature. Basic ideas for
appreciating and understanding generalizations about
AI are introduced. Those include eight evaluation issues
related to generalizations and a series of concepts
related to AI, algorithms, and digital agents. The work
system perspective is summarized as a way to describe
the use of AI-based digital agents in specific application
situations. An implied series of concerns about AI-based
digital agents is identified. Six AI-related examples are
introduced and then used in combination to illustrate the
use of the work system perspective for visualizing the
danger of trying to generalize about AI without saying
which AI-related capabilities are being discussed, what
is the context of use, and what roles human experience,
URI: https://hdl.handle.net/10125/107090
978-0-9981331-7-1
(CC BY-NC-ND 4.0)
Page 5856
intuition, values, and insights play in AI usage. The
concluding section presents suggestions for realistic
descriptions of AI applications and for generalizations
about AI in its context of usage.

## 2. Definitions and generalizations about AI
Below are definitions and generalizations about AI
published during 2019-2022, i.e., literature that is recent
enough to reflect current capabilities rather than
ambitions or predictions from decades ago. Thus, we
will not discuss decades of AI research going back to the
famous Dartmouth workshop in 1956, which defined the
project of creating AI in terms of “making a machine
behave in ways that would be called intelligent if a
human were so behaving” (McCarthy et al. 1955).
Many other definitions and generalizations could
have been listed. While the following statements are
taken out of context and therefore do not fully represent
the sources where they appeared, disparities between
these recent statements about AI imply that a reasonably
consistent way to talk about AI-based systems could be
a step forward in understanding AI-related phenomena.
- “a system’s ability to interpret external data
correctly, to learn from such data, and to use those
learnings to achieve specific goals through flexible
adaptation” (Haenlein & Kaplan, 2019, p. 5),
- “the ability of a machine to perform cognitive
functions that we associate with human minds, such
as perceiving, reasoning, learning, interacting with
the environment, problem solving, decision-
making, and even demonstrating creativity” (Rai et
al., 2019, p. iii)
- “a highly capable and complex technology that
aims to simulate human intelligence” (Glikson &
Woolley, 2020, p. 627)
- a general-purpose technology (GPT) with a unique
learning capability that provides organizations with
potentials for wide-ranging improvements as well
as entirely new business opportunities.” (Jöhnk et
al., 2021, p. 5)
- “AI systems have unique characteristics that go
beyond automation. First, AI systems have the
potential to substitute for entire work processes.
Second, they can eliminate employees’ interaction
possibilities. Third, they can learn and thus may
derive unpredictable work outcomes, and fourth,
they are often not transparent to employees and
therefore lack explainability.” (Strich et al., 2021,
p. 306)
- “the frontier of computational advancements that
references human intelligence in addressing ever
more complex decision-making problems. In short,
AI is whatever we are doing next in computing.”
(Berente et al., 2021, p. 1435)
- “the ability of a system to identify, interpret, make
inferences, and learn from data to achieve
predetermined organizational and societal goals”
(Mikalef, et al., 2022, p. 258).

Some of the above statements from 2019-2022
seem to be anchored in dreams and ambitions from
decades ago. Most seem vague and non-operational.
Some automated systems exhibit some of the
capabilities that are included within these broad
definitions, but most automated systems that are
associated with AI and that operate in the world do not
come close to the human-like level of thinking, learning,
or solving problems that is implied by some of these
definitions. For example, regardless of how many times
AI is described as mimicking human problem solving,
the algorithms that drive current automated systems do
not apply human-like experience, knowledge, intuition,
values, or ethics, as has been stated many times and as
will be demonstrated later through six examples.

## 3. Basic ideas for appreciating and understanding
generalizations about AI

This section presents basic ideas for appreciating
and understanding generalizations about AI. Section 3.1
presents a set of evaluation issues related to
generalizations. Section 3.2 presents ideas that are
directly useful for understanding the development of AI-
related capabilities and their use in specific situations.

### 3.1 Evaluation issues related to generalizations
A series of evaluation issues for generalizations
were developed while exploring the IS field’s
discussions and controversies related to theories and
theorizing (e.g., Avison & Malaurent (2014) and related
debate responses about whether theory is a fetish in IS,
Rivard (2014), Grover & Lyytinen (2015), Hirschheim
(2019) and a related debate article about whether the IS
field has a theory fixation (Hovorka et al., 2019), and a
JAIS editorial about products of theorizing (Hassan et
al., 2022). Comparing those articles and others led to a
belief that the IS field might benefit from shifting its
focus away from theory per se and toward the broader
idea of generalizations, which was discussed in depth by
Lee & Baskerville (2003). The current discussion looks
at generalizations in a simpler way that uses typical
ideas for evaluating theories, such as conceptual clarity
and domain clarity.
The following eight evaluation issues can be
applied to almost any generalization while trying to
understand what it says and whether it is convincing.
Page 5857
Implied or actual definitions and generalizations about
AI, RAI, and TAI in many discussions of those topics
do not fare well when evaluated based on these issues.
A brief comment in parenthesis for each issue reflects
how the issue applies to a part (shown in quotations) of
one or more of the AI generalizations mentioned earlier.
- **Conceptual clarity.** How clearly defined are any
concepts, associations, symbols, or computations
whose definition is not obvious? (What is a
“general purpose technology”?)
- **Domain clarity.** How clear is the boundary
between situations covered by the generalization
fully vs. situations covered partially, or not at all?
(The limits of large language models as “stochastic
parrots” that extend word strings based on
probabilities (Bender et al., 2021) conflicts with the
notion that AI can “learn from data to achieve
predetermined organizational and societal goals.”)
- **Time span clarity.** During which time span is the
generalization most applicable and when is it less
applicable or irrelevant? (Use of machine learning
has overcome disillusionment about neural
networks from decades ago. It would be interesting
to see how generalizations related to neural
networks have changed over time.)
- **Omissions.** What possibly relevant factors,
concepts, issues, circumstances, or occurrences are
not covered by the generalization or its rationale?
(What about realities, conditions, or issues not
touched by training data used “to achieve specific
goals through flexible adaptation”?)
- **Examples.** What nontrivial examples in the
generalization’s domain illuminate its meaning?
What counterexamples illuminate its boundaries?
(Which examples show that AI applications
typically “aim to simulate human intelligence.”)
- **Parsimony.** What parts of the generalization seem
to be unnecessary? (Is it generally true that AI is a
“frontier of computational advancements that
references human intelligence in addressing ever
more complex decision-making problems”?)
- **Justification.** How convincing is the justification
of the generalization based on a path of empirical,
statistical, and/or conceptual reasoning applied to
convincing examples in the domain? (Original
argumentation supporting the quotations could not
be included here, but it seems likely that justifying
some of the definitions would be challenging.)
- **Value.** How could the generalization help people
meet practical or scientific goals? In what ways is
it more useful than other generalizations that touch
on related topics? (What is the value of saying that
“AI systems have the potential to substitute for
entire work processes” when many parts of those
processes will be performed by human actors or
nonhuman actors not directly related to AI?)

Validation of a generalization should address those
evaluation issues to whatever extent is appropriate for
the purpose at hand. Validation may involve logical
analysis, statistical analysis, analysis of instances, key
examples, scarcity of counterexamples, widely accepted
wisdom, assertions by trusted authorities, and so on.

### 3.2 Concepts for describing AI applications
This section identifies concepts that can be used in
describing the development and application of AI-based
capabilities. It starts with definitions related to
algorithms and digital agents, mostly from Alter (2023).
Those concepts can be used for describing AI
applications as isolated entities. Next is a summary of
aspects of a work system perspective that frames the
context for using AI in operational real-world systems.

**Algorithm.** A set of instructions for achieving
specified goals within stated or unstated constraints by
applying specific resources such as data inputs.
Algorithms may be as simple as decision rules or as
complex as integrated algorithms for self-driving cars.
As abstractions, algorithms cannot do anything by
themselves and have effect only when human or non-
human actors use them to support, control, or perform
actions in the world.

**Agent.** An entity that performs task(s) delegated by
another entity or group of entities that may be human or
nonhuman. This definition assumes that actors, (entities
that perform actions) may or may not be agents.

**Algorithmic agent.** A physical or digital agent that
operates by executing explicit algorithms. Algorithmic
agents may be human or nonhuman.

**Digital agent.** An algorithmic agent that operates by
executing algorithms encoded in software and that has
no persistent physical existence.

**AI-based digital agent.** A digital agent whose main
activities are guided by algorithms created using
techniques associated with AI. Those techniques might
be related to various aspects of machine learning (ML),
large language models (LLMs), natural language
processing, automation of decision processes, capture of
abstract knowledge, or many other types of techniques.

**Activity.** A purposeful action that is significant
enough to identify when trying to understand a system’s
development or operation.

**Work system.** A system in which human
participants and/or machines perform work (processes
and activities) using information, technology, and other
resources to produce specific product/services for
internal and/or external customers (Alter, 2006; 2013).

**AI-based digital agent in its context of usage.** Most
significant uses of AI-based digital agents occur as a
work system’s delegation of roles and responsibilities to
those AI-based digital agents. This can be understood for
analysis purposes as delegation by a work system whose
human participants or totally automated components
may initiate, control, or terminate that delegation.

**Definition of AI.** This paper accepts the suggestion
in Berente et al. (2021) that accepting diverse views of
AI may lead to “productive inquiry and technological
advancements.” It proceeds by assuming that AI usage is
the usage of digital agents created by using AI-related
techniques regardless of how AI is defined.

Box 1 in Alter (2023) illustrates the difficulty of
generalizing about benefits, risks, and ethics of AI
without explaining details of specific AI applications
that are being described. It is doubtful that any non-
trivial statements about likely benefits, risks, and ethics
of AI in general could be derived from analyzing AI
applications in examples that use different techniques in
different application areas with vastly different impacts
of possible design errors or other shortcomings.

## 4. Using the work system perspective to
frame the use of AI-based digital agents
The idea of work system (WS) provides a way to
identify the context within which algorithms are
applied, regardless of whether those algorithms were
developed using AI-related methods. The core of the
work system perspective has been presented many times
(Alter, 2006, 2008, 2013). The following summary of
selected aspects of that perspective emphasizes ideas
that are directly useful in understanding the use of AI-
based digital agents.

**Performing work in work systems as the context
for applying AI.** In an economic context, work is the
application of human, informational, physical, and other
resources to produce product/services for internal or
external customers. Work occurs in homes, businesses,
governments, and other situations where purposeful use
of resources aims to produce outcomes. The first and/or
in the definition of work system says that work systems
may be sociotechnical (with human participants doing
some of the work) or totally automated.

**Special cases of work system.** Digital agents (as
described above) are work systems. An IS is a WS most
of whose activities are devoted to capturing,
transmitting, storing, retrieving, deleting, manipulating,
and/or displaying information. Projects such as software
development projects are WSs designed to produce
specific product/services and then go out of existence.

**Work system framework: a basic understanding
of a work system.** The nine elements of the work system
framework are the elements of a basic understanding of
a WS’s form, function, and environment during a period
when it is stable enough to retain its identity even though
incremental changes may occur, such as minor personnel
substitutions or technology upgrades. Those elements
include customers, product/services, processes and
activities, participants, information, technologies,
environment, infrastructure, and strategies. Arrows
between elements of the work system framework imply
the importance of alignment between the elements.

**Work system life cycle model (WSLC): how WSs
change over time.** ISs and other WSs evolve through a
combination of planned change through projects
involving initiation, development, implementation and
unplanned change via adaptations and workarounds. The
WSLC phases may be performed in many ways.
Activities and responsibilities associated with specific
phases (e.g., designing, debugging, training, etc.) apply
for waterfall, agile, prototyping, use of off-the-shelf
applications, and shadow IT, even when several phases
overlap or are combined through short iterations.

**Roles of digital agents.** The agent-responsibility
framework discussed in Alter (2023) identifies six types
of roles that digital agents may perform for a work
system. Those roles appear along a spectrum from the
lowest to the highest degree of direct involvement of the
digital agent in the execution of a work system’s
activities. The steps along that spectrum include:
- monitoring the WS but not performing any of its
work,
- providing information that the WS uses,
- providing capabilities that the WS uses,
- controlling activities within the WS
- coproducing activities (division of labor involving
complementary responsibilities of people and
digital agents for performing their parts of the work)
- executing activities (the digital agent performs
activities that should not or cannot be performed by
WS participants).

**Facets of work.** Going even deeper, the roles of
digital agents may be directed at different facets of work
in a work system. Alter (2021) identifies the 18 facets of
work shown in Table 1 and explains why they were
identified as noteworthy. Digital agents may address any
of those 18 facets of work (Alter, 2023) within a work
system.

**Table 1. 18 facets of work**
| Facets of work          |
|-------------------------|
| Making decisions        |
| Communicating           |
| Providing information   |
| Representing reality    |
| Applying knowledge      |
| Thinking                |
| Learning                |
| Planning                |
| Controlling execution   |
| Coordinating            |
| Improvising             |
| Processing information  |
| Performing physical work|
| Performing support work |
| Interacting socially    |
| Providing service       |
| Creating value          |
| Maintaining security    |

## 5. Concerns about AI-based digital agents implied
by the work system perspective
The work system perspective leads directly to a series
of concerns regarding the evolution and use of AI-based
digital agents. The concerns shown in Tables 2 and 3 are
organized around the four phases of the work system life
cycle model and the elements of the work system
framework, respectively. Many concerns related to the
operation of the work system (i.e., Table 3) are similar to
or identical to concerns related to the evolution and use
of almost any software-related tool or capability that
plays a role in a work system. HCI guru Ben
Shneiderman seemed to agree with that view when asked
whether human-centered AI (HCAI) was really about AI
or whether it was about all of technology, he said that
HCAI ideas apply to technology in general but that the
current interest in AI is so enormous and so pervasive
that the focus should be on human-centered AI (at 1:07
in Shneiderman, 2021).

AlphaFold operates based on a database of known
protein structures and predicts the likely structure of
other proteins. (Jumper et al., 2021). DeepMind
provides “programmatic access to and interactive
visualization of predicted atomic coordinates, per-
residue and pairwise model-confidence estimates and
predicted aligned errors.” (Varadi et al., 2022, p. D439).
This example involves multiple work systems. The
most reported work system is a highly iterative project
of producing a tool based on machine learning that
generates predictions of the structure of proteins. The
others are the individual work systems of various
research groups that use capabilities (AI-based digital
agents) made available by AlphaFold to pursue their
own research. In the first case, the work system life
cycle model could be used to describe how the algorithm
was envisioned (the initiation phase), how it was
developed using machine learning technologies and
then programmed as a digital agent (development), how
scientists tested whether it worked (implementation,
although not implementation in an organization), and
how it was used (operation). The work system
framework could be used to describe the project in terms
of participants, relevant information and technology,
process of producing results, product/services produced,
and so on. In applications of the resulting tools, the work
systems of individual research groups include their
exploration of AlphaFold capabilities and their use of
AlphaFold in pursuing their own research objectives.

## 6. Examples illustrating uses of AI-based
digital agents
Six examples will be presented to illustrate quite
different current applications of AI-based digital agents.
These examples were selected to illustrate that AI-
related techniques have been used in a wide range of
applications and that their use may succeed or fail to
varying extents. Five examples are from recent sources
that are cited. The sixth is several aspects of my
application of ChatGPT as a useful but highly fallible
idea generator while developing research ideas. Brief
comments based on the content of Sections 4 and 5 will
illustrate how a work system perspective can be applied
to each case. Throughout this section notice that
applications of AI can be described without relying on
controversial notions such as machine intelligence or
human-like problem solving capabilities.

### 6.1 Scientific breakthrough in predicting protein-
folds
Understanding how a specific protein’s sequence of
amino acids folds in three dimensions is important for
developing drugs and for understanding biological
structures more broadly. DeepMind, part of Google’s
parent company, produced a machine learning
application called AlphaFold that vastly outperforms
previous algorithms in generating predictions about the
likely structure of proteins containing specific
sequences of amino acids. Around 100,000 of those
structures had been determined with enormous effort,
but billions of possibilities had not been explored.

### 6.2 Using diagnostic radiology tools
Lebovitz et al. (2021) presents a field study of
radiologist-managers evaluating five previously existing
diagnostic radiology tools in pilot studies for possible
adoption as components of a major hospital’s medical
work systems. The five tools were developed using
machine learning (ML), a branch of current AI research
and practice that has many important applications.
Although five ML tools proved “highly accurate, all five
tools performed poorly during internal pilot studies”
[leading] “managers to confront the high uncertainty
involved in evaluating human experts’ knowledge
outputs (know-what) and to recognize that ML-based AI
tools did not capture experts’ tacit knowledge practices
(know-how).” A key issue was that the use of radiology
images is part of a diagnostic process that extends over
time and often uses multiple images and much other
medical knowledge. Isolated ML-based predictions that
are produced using single images did not fit well into the
actual work systems of radiologists. Ongoing trials were
aimed at finding ways to use the ML-based tools
effectively.

### 6.3 Self-driving vehicles
Self-driving vehicles present many challenges
associated with using AI to capture and use real time
information of various types for making decisions about
where to go and how to proceed safely. That information
concerns other vehicles that may be moving or parked,
pedestrians, cyclists, and other possible obstacles, road
and lane markings, traffic signs and signals, road and
weather conditions, traffic flows, and likely trajectories
of anything whose movement might be relevant,
including vehicles whose driver’s intent is not known.
Those challenges have been apparent for years even
as various technologies advanced greatly. Data from the
National Highway Traffic Safety Administration
identified 736 U.S. crashes and 17 fatalities since 2019
involving Teslas in Autopilot mode. (Siddiqui and
Merrill, 2023). Three incidents occurred in San
Francisco just days after the California Public Utilities
Commission gave Waymo and Cruise permission to
offer paid rides anytime during the day throughout the
city. Ten of those cars stopped working and caused a
traffic jam when their communication system
overloaded due to a music festival; a driverless car
collided with an emergency vehicle; and a driverless car
got stuck in wet concrete in a construction zone
(Levenson, 2023).
The primary work system in these instances is
driving from one location to another. Subsystems of that
work system capture and consolidate real time data,
make projections, determine and adjust routes, and
control the vehicle in real time. Other work systems
include central monitoring and control of fleets of
driverless cars, hailing driverless cars, and paying for
trips.

### 6.4 Preventing access by unauthorized
individuals
A lawyer chaperoning her daughter’s Girl Scout
troop to see a show at Radio City Music Hall in New
York City was pulled aside and denied entry to the show
because her law firm was involved in litigation against
the owner of that venue. Lawyers working for firms
pursuing litigation against the owners had been placed
on an exclusion list. Their photos had been scraped from
their law firms’ websites, and her photo matched a photo
taken by a surveillance camera at the entry point for the
show. That use of facial recognition and similar uses
that prevented lawyers from using their tickets to
basketball games at Madison Square Garden caused an
uproar. (Restrepo, 2023).
The work system in this instance involved
controlling access to entertainment events. In effect, the
owners of the venue dictated the use of a business rule
that lawyers working for certain law firms would not be
admitted. The digital agent compared a camera image of
each ticket-holder with images scraped previously from
websites. This is an instance of facial recognition, which
is often associated with AI. Whether or not this specific
instance involved AI usage is unclear because it is
possible to talk about matching images as a statistical
calculation without mentioning AI. In relation to roles
and facets of work, the digital agent played the role of
providing information that was used by human
participants for facets of work including making
decisions and controlling execution. Similar processes
might be used for the facet of work “maintaining
security,” but in this instance the goal was not really
about security.

### 6.5 Producing a legal brief
A lawyer for a plaintiff who had been injured by a
serving cart on an Avianca flight objected to a request
by Avianca that a federal judge should reject the case.
That lawyer submitted a legal brief that cited seemingly
relevant court decisions, such as “Martinez v. Delta Air
Lines, Zicherman v. Korean Air Lines and, of course,
Varghese v. China Southern Airlines, with its learned
discussion of federal law and ‘the tolling effect of the
automatic stay on a statute of limitations.’” Neither
Avianca’s lawyers nor the judge could find those
decisions or quotations. The plaintiff’s lawyer had tried
to use ChatGPT to find relevant legal precedents and did
not realize that ChatGPT was not an advanced search
engine. (Weiser, 2023a). The lawyer did not understand
that ChatGPT is a large language model that produces
text by using probabilities to select among possible next
words based on its training data. That process is
explained in the mathematician Stephen Wolfram’s
Page 5862
(2023) lengthy blog post about how ChatGPT operates.
The lawyer’s subsequent declaration to the federal court
judge said “I simply had no idea that ChatGPT was
capable of fabricating entire case citations or judicial
opinions, especially in a manner that appeared
authentic” (Weiser, 2023b)
The work system in this case was performing legal
research as part of producing legal briefs. The lawyer
tried to use ChatGPT as a shortcut. Careless use of
ChatGPT’s responses led to the lawyer’s production of
a nonsensical legal brief that was presented to a federal
judge. In this instance, ChatGPT played the role of
providing information (which proved incorrect). It did
not make decisions about the content of the legal brief.
That was the lawyer’s responsibility.

### 6.6 Idea generator for IS research
In the months before writing this paper, I applied
ChatGPT as a useful but highly limited idea generator
despite its widely discussed limitations as a large
language model (LLM). LLMs produce text by using
probabilities to determine the next word in sentences
based on next-word probabilities in a huge set of
sentences scraped from the Internet. LLMs have no
knowledge of any context and therefore have a tendency
to “hallucinate” (see Berghel (2023) and Wolfram
(2023)) and to produce statements that have no moral
basis (see Chomsky et al. (2023)). Here are two
examples of using ChatGPT as an idea generator.
**Exploring axioms related to IS.** I used ChatGPT
while trying to identify axioms that apply to every work
system. The following three statements about customers
are ChatGPT responses to queries requesting axioms
related elements of the work system framework: 
1. “The customer is always right,” 
2. “customer retention is as important as customer acquisition,” 
3. “customer feedback is essential for improving product/services and
maintaining customer satisfaction.” 
Those sentences
generally make sense in relation to discussions about
customers, but in this instance ChatGPT did not
“recognize” that customer was to be understood in
relation to the work system framework, where a work
system’s customers receive and use product/services
that it produces, may be internal customers, and may be
totally automated. Those ChatGPT responses and many
other responses proved useful as test cases for
considering what might or might not qualify as axioms
in relation to the work system framework. Borderline
responses were especially helpful in thinking about what
might qualify.

**Producing a list of workarounds.** ChatGPT
helped in preparing for a workshop on workarounds by
responding to queries such as “Identify 20 common
workarounds related to software development” or “20
common workarounds related to nursing.” When asked
to identify financial workarounds, ChatGPT stayed on
safe ground by saying that it was an LLM and could not
suggest anything illegal. Its responses to a modified
query about “legal financial workarounds” included the
common workaround of “creating multiple smaller
invoices instead of one large invoice to bypass the
company's approval process for large expenses.” Other
responses such as “using personal bank accounts” might
have been workarounds or ChatGPT hallucinations.
Those questionable responses were potentially useful as
examples in discussions about what qualifies as a
workaround. More broadly, even though workaround
was defined carefully in the queries, ChatGPT was
unable to differentiate between maneuvers that typically
would be viewed as workarounds versus minor
common- sense adjustments to unexpected conditions.

### 6.7 Discussion of the examples
The six examples were chosen to illustrate the range
of possibilities in using AI-based digital agents. The
protein-fold and radiological diagnosis examples were
serious scientific efforts to use machine learning to
address important problems that were either difficult or
impossible for people. People involved in those projects
used deep knowledge to think about how an AI-based
digital agent might be used and to evaluate the results of
that use. The ethical issues in those cases were about the
accuracy and understandability of the answers that were
produced, not about mistreatment due to bias or lack of
privacy. The self-driving vehicle example combines
numerous technical challenges related to capturing and
using real time data plus serious ethical issues about
dangers from using self-driving vehicles before the
technology was perfected. Also, this example illustrates
the importance of looking at the context of use and the
surrounding environment when designing or evaluating
AI-related capabilities. The facial recognition case
raises ethical issues that are less related to use of
technology than to the ethics of excluding people from
entertainment events based on their employer. The two
ChatGPT examples showed that the trustworthiness or
untrustworthiness of an AI-based digital agent can have
vastly different ramifications based on the context and
the knowledge of work system participants. ChatGPT
was not responsible for the outcome in the legal brief
case even though it produced information that was
incorrect. The use of ChatGPT to generate ideas for
research occurred with full awareness that it could
propose incorrect ideas whose borderline nature might
be useful in thinking about research ideas. In general,
people who use a tool of any type, regardless of whether
it is based on AI, should understand its capabilities in
relation to the desired application.

In combination, the six examples demonstrate the
danger of trying to generalize about AI without saying
which type of AI was being discussed, what was the
context of use, and what roles human experience,
intuition, values, and insights play in AI usage.

## 7. Conclusion: Describing AI applications and
generalizing about AI
The great importance and high visibility of many
forms of AI research and application call for careful
discussions of AI even though AI has turned into an
umbrella term covering fundamentally different
phenomena and application areas. Those circumstances
make it all the more important to apply organized ideas
in designing, analyzing, and evaluating work systems
that use AI-based digital agents. Similarly, it is
important to cast a critical eye on generalizations about
AI to assure that they involve more than ambitions,
hype, marketing, or science fiction imagery.
This paper addressed those issues by questioning
selected definitions of AI, proposing evaluation issues
related to generalizations, presenting ideas that are
useful for describing AI applications (algorithm, digital
agent, work system, role of digital agent, facet of work),
and applying those ideas to six AI-related examples that
illustrate the diversity of situations in which AI-related
techniques have been used. That coverage leads to the
following suggestions for describing uses of AI and for
generalizations about AI.

### 7.1 Suggestions about describing uses of AI
1) Recognize the difference between the
development, operation, and evaluation of an AI-based
digital agent and the development, operation, and
evaluation of a work system that uses that digital agent.
The work system perspective applies at both levels and
helps in clarifying the subject of AI-related
generalizations.
2) The description, analysis, design, or evaluation
of anything called an AI application should identify the
relevant digital agents and their context of use. The
work system perspective provides a plausible approach
for pursuing that purpose for AI-based digital agents
applied in organizational settings and infrastructures.
3) Most real-world issues about AI in operation are
fundamentally similar to real-world issues related to
important work systems, including information systems.
For example, a hiring system that is based totally on a
manager’s experience and preferences (i.e., the
manager’s biases) is probably as likely to exhibit bias as
a totally automated hiring system based totally on a
firm’s past experience with applicants (which may give
greater weight to characteristics of some groups
rather than others). Manipulating training data to emphasize
some characteristics and not others simply introduces
another type of bias, regardless of whether that bias
might seem beneficial to some stakeholders.

### 7.2 Suggestions about generalizations about AI
1) Context matters. Context-free generalizations
about the power, benefits, or dangers of AI make no
more sense than most context-free generalizations about
the power, benefits, or dangers of computers or
software.
2) Generalizations about AI, RAI, and TAI should
clarify which groups of instances are being discussed
and which possibly relevant instances are not being
discussed. For example, important privacy and bias
concerns related to applications of AI to select some
people over others are not relevant to many natural
science applications of AI.
3) Generalizations of the form “AI can do X” or “IT
can do Y” tend to be inherently misleading because
applications of IT and AI include a multitude of
instances with vastly different properties. The fact that
an AI-based digital agent can perform facial recognition
is irrelevant if the problem at hand is drug discovery or
detection of financial improprieties. Also, the spectrum
of different roles that AI-based digital agents might play
(from monitoring work or providing information
through coproducing activities and executing activities)
illustrates that saying that an instance of AI usage
involves one type of role does not imply that AI usage
might be useful in other types of roles.
4) Avoid using the term “AI system” in operational
settings because most operational systems that use AI
are not inherently about AI. E.g., a facial recognition
system is about facial recognition, not about AI. Calling
something an AI system because it uses AI-related
techniques is somewhat similar to calling a work system
a computer system because it uses computers.
5) Avoid pretending that AI thinks like humans
when it obviously does not (e.g., see Chomsky et al.,
2023). In particular, avoid pretending that AI methods
that are basically large statistical algorithms reflect
human thought processes even if those algorithms might
address similar problems (classifying, predicting,
making decisions) and might produce similar results.
6) Most broadly, the inclusion of many disparate
computing applications under the heading of AI
undermines attempts to generalize meaningfully about
AI capabilities. As a tremendously important topic, AI
deserves the same kind of careful attention and clarity
that is devoted to important topics in many other areas.